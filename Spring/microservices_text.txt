File Loaded
pdf_images exists/created
pdf_images/page_1.png saved
extracted text from : pdf_images/page_1.png
pdf_images/page_2.png saved
extracted text from : pdf_images/page_2.png
pdf_images/page_3.png saved
extracted text from : pdf_images/page_3.png
pdf_images/page_4.png saved
extracted text from : pdf_images/page_4.png
pdf_images/page_5.png saved
extracted text from : pdf_images/page_5.png
pdf_images/page_6.png saved
extracted text from : pdf_images/page_6.png
pdf_images/page_7.png saved
extracted text from : pdf_images/page_7.png
pdf_images/page_8.png saved
extracted text from : pdf_images/page_8.png
pdf_images/page_9.png saved
extracted text from : pdf_images/page_9.png
pdf_images/page_10.png saved
extracted text from : pdf_images/page_10.png
pdf_images/page_11.png saved
extracted text from : pdf_images/page_11.png
pdf_images/page_12.png saved
extracted text from : pdf_images/page_12.png
pdf_images/page_13.png saved
extracted text from : pdf_images/page_13.png
pdf_images/page_14.png saved
extracted text from : pdf_images/page_14.png
pdf_images/page_15.png saved
extracted text from : pdf_images/page_15.png
pdf_images/page_16.png saved
extracted text from : pdf_images/page_16.png
pdf_images/page_17.png saved
extracted text from : pdf_images/page_17.png
pdf_images/page_18.png saved
extracted text from : pdf_images/page_18.png
pdf_images/page_19.png saved
extracted text from : pdf_images/page_19.png
pdf_images/page_20.png saved
extracted text from : pdf_images/page_20.png
pdf_images/page_21.png saved
extracted text from : pdf_images/page_21.png
pdf_images/page_22.png saved
extracted text from : pdf_images/page_22.png
pdf_images/page_23.png saved
extracted text from : pdf_images/page_23.png
pdf_images/page_24.png saved
extracted text from : pdf_images/page_24.png
pdf_images/page_25.png saved
extracted text from : pdf_images/page_25.png
pdf_images/page_26.png saved
extracted text from : pdf_images/page_26.png
pdf_images/page_27.png saved
extracted text from : pdf_images/page_27.png
pdf_images/page_28.png saved
extracted text from : pdf_images/page_28.png
pdf_images/page_29.png saved
extracted text from : pdf_images/page_29.png
pdf_images/page_30.png saved
extracted text from : pdf_images/page_30.png
pdf_images/page_31.png saved
extracted text from : pdf_images/page_31.png
pdf_images/page_32.png saved
extracted text from : pdf_images/page_32.png
pdf_images/page_33.png saved
extracted text from : pdf_images/page_33.png
pdf_images/page_34.png saved
extracted text from : pdf_images/page_34.png
pdf_images/page_35.png saved
extracted text from : pdf_images/page_35.png
pdf_images/page_36.png saved
extracted text from : pdf_images/page_36.png
pdf_images/page_37.png saved
extracted text from : pdf_images/page_37.png
pdf_images/page_38.png saved
extracted text from : pdf_images/page_38.png
pdf_images/page_39.png saved
extracted text from : pdf_images/page_39.png
pdf_images/page_40.png saved
extracted text from : pdf_images/page_40.png
pdf_images/page_41.png saved
extracted text from : pdf_images/page_41.png
pdf_images/page_42.png saved
extracted text from : pdf_images/page_42.png
pdf_images/page_43.png saved
extracted text from : pdf_images/page_43.png
pdf_images/page_44.png saved
extracted text from : pdf_images/page_44.png
pdf_images/page_45.png saved
extracted text from : pdf_images/page_45.png
pdf_images/page_46.png saved
extracted text from : pdf_images/page_46.png
pdf_images/page_47.png saved
extracted text from : pdf_images/page_47.png
pdf_images/page_48.png saved
extracted text from : pdf_images/page_48.png
pdf_images/page_49.png saved
extracted text from : pdf_images/page_49.png
pdf_images/page_50.png saved
extracted text from : pdf_images/page_50.png
pdf_images/page_51.png saved
extracted text from : pdf_images/page_51.png
pdf_images/page_52.png saved
extracted text from : pdf_images/page_52.png
pdf_images/page_53.png saved
extracted text from : pdf_images/page_53.png
pdf_images/page_54.png saved
extracted text from : pdf_images/page_54.png
pdf_images/page_55.png saved
extracted text from : pdf_images/page_55.png
pdf_images/page_56.png saved
extracted text from : pdf_images/page_56.png
pdf_images/page_57.png saved
extracted text from : pdf_images/page_57.png
pdf_images/page_58.png saved
extracted text from : pdf_images/page_58.png
pdf_images/page_59.png saved
extracted text from : pdf_images/page_59.png
pdf_images/page_60.png saved
extracted text from : pdf_images/page_60.png
pdf_images/page_61.png saved
extracted text from : pdf_images/page_61.png
pdf_images/page_62.png saved
extracted text from : pdf_images/page_62.png
pdf_images/page_63.png saved
extracted text from : pdf_images/page_63.png
pdf_images/page_64.png saved
extracted text from : pdf_images/page_64.png
pdf_images/page_65.png saved
extracted text from : pdf_images/page_65.png
pdf_images/page_66.png saved
extracted text from : pdf_images/page_66.png
pdf_images/page_67.png saved
extracted text from : pdf_images/page_67.png
pdf_images/page_68.png saved
extracted text from : pdf_images/page_68.png
pdf_images/page_69.png saved
extracted text from : pdf_images/page_69.png
pdf_images/page_70.png saved
extracted text from : pdf_images/page_70.png
pdf_images/page_71.png saved
extracted text from : pdf_images/page_71.png
pdf_images/page_72.png saved
extracted text from : pdf_images/page_72.png
pdf_images/page_73.png saved
extracted text from : pdf_images/page_73.png
pdf_images/page_74.png saved
extracted text from : pdf_images/page_74.png
pdf_images/page_75.png saved
extracted text from : pdf_images/page_75.png
pdf_images/page_76.png saved
extracted text from : pdf_images/page_76.png
pdf_images/page_77.png saved
extracted text from : pdf_images/page_77.png
pdf_images/page_78.png saved
extracted text from : pdf_images/page_78.png
pdf_images/page_79.png saved
extracted text from : pdf_images/page_79.png
pdf_images/page_80.png saved
extracted text from : pdf_images/page_80.png
pdf_images/page_81.png saved
extracted text from : pdf_images/page_81.png
pdf_images/page_82.png saved
extracted text from : pdf_images/page_82.png
pdf_images/page_83.png saved
extracted text from : pdf_images/page_83.png
pdf_images/page_84.png saved
extracted text from : pdf_images/page_84.png
pdf_images/page_85.png saved
extracted text from : pdf_images/page_85.png
pdf_images/page_86.png saved
extracted text from : pdf_images/page_86.png
pdf_images/page_87.png saved
extracted text from : pdf_images/page_87.png
pdf_images/page_88.png saved
extracted text from : pdf_images/page_88.png
pdf_images/page_89.png saved
extracted text from : pdf_images/page_89.png
pdf_images/page_90.png saved
extracted text from : pdf_images/page_90.png
pdf_images/page_91.png saved
extracted text from : pdf_images/page_91.png
pdf_images/page_92.png saved
extracted text from : pdf_images/page_92.png
pdf_images/page_93.png saved
extracted text from : pdf_images/page_93.png
pdf_images/page_94.png saved
extracted text from : pdf_images/page_94.png
pdf_images/page_95.png saved
extracted text from : pdf_images/page_95.png
pdf_images/page_96.png saved
extracted text from : pdf_images/page_96.png
pdf_images/page_97.png saved
extracted text from : pdf_images/page_97.png
pdf_images/page_98.png saved
extracted text from : pdf_images/page_98.png
pdf_images/page_99.png saved
extracted text from : pdf_images/page_99.png
pdf_images/page_100.png saved
extracted text from : pdf_images/page_100.png
pdf_images/page_101.png saved
extracted text from : pdf_images/page_101.png
pdf_images/page_102.png saved
extracted text from : pdf_images/page_102.png
pdf_images/page_103.png saved
extracted text from : pdf_images/page_103.png
pdf_images/page_104.png saved
extracted text from : pdf_images/page_104.png
pdf_images/page_105.png saved
extracted text from : pdf_images/page_105.png
pdf_images/page_106.png saved
extracted text from : pdf_images/page_106.png
pdf_images/page_107.png saved
extracted text from : pdf_images/page_107.png
pdf_images/page_108.png saved
extracted text from : pdf_images/page_108.png
pdf_images/page_109.png saved
extracted text from : pdf_images/page_109.png
pdf_images/page_110.png saved
extracted text from : pdf_images/page_110.png
pdf_images/page_111.png saved
extracted text from : pdf_images/page_111.png
pdf_images/page_112.png saved
extracted text from : pdf_images/page_112.png
pdf_images/page_113.png saved
extracted text from : pdf_images/page_113.png
pdf_images/page_114.png saved
extracted text from : pdf_images/page_114.png
pdf_images/page_115.png saved
extracted text from : pdf_images/page_115.png
pdf_images/page_116.png saved
extracted text from : pdf_images/page_116.png
pdf_images/page_117.png saved
extracted text from : pdf_images/page_117.png
pdf_images/page_118.png saved
extracted text from : pdf_images/page_118.png
pdf_images/page_119.png saved
extracted text from : pdf_images/page_119.png
pdf_images/page_120.png saved
extracted text from : pdf_images/page_120.png
pdf_images/page_121.png saved
extracted text from : pdf_images/page_121.png
pdf_images/page_122.png saved
extracted text from : pdf_images/page_122.png
pdf_images/page_123.png saved
extracted text from : pdf_images/page_123.png
pdf_images/page_124.png saved
extracted text from : pdf_images/page_124.png
pdf_images/page_125.png saved
extracted text from : pdf_images/page_125.png
pdf_images/page_126.png saved
extracted text from : pdf_images/page_126.png
pdf_images/page_127.png saved
extracted text from : pdf_images/page_127.png
pdf_images/page_128.png saved
extracted text from : pdf_images/page_128.png
pdf_images/page_129.png saved
extracted text from : pdf_images/page_129.png
pdf_images/page_130.png saved
extracted text from : pdf_images/page_130.png
pdf_images/page_131.png saved
extracted text from : pdf_images/page_131.png
pdf_images/page_132.png saved
extracted text from : pdf_images/page_132.png
pdf_images/page_133.png saved
extracted text from : pdf_images/page_133.png
pdf_images/page_134.png saved
extracted text from : pdf_images/page_134.png
pdf_images/page_135.png saved
extracted text from : pdf_images/page_135.png
pdf_images/page_136.png saved
extracted text from : pdf_images/page_136.png
pdf_images/page_137.png saved
extracted text from : pdf_images/page_137.png
pdf_images/page_138.png saved
extracted text from : pdf_images/page_138.png
pdf_images/page_139.png saved
extracted text from : pdf_images/page_139.png
pdf_images/page_140.png saved
extracted text from : pdf_images/page_140.png
pdf_images/page_141.png saved
extracted text from : pdf_images/page_141.png
pdf_images/page_142.png saved
extracted text from : pdf_images/page_142.png
pdf_images/page_143.png saved
extracted text from : pdf_images/page_143.png
pdf_images/page_144.png saved
extracted text from : pdf_images/page_144.png
pdf_images/page_145.png saved
extracted text from : pdf_images/page_145.png
pdf_images/page_146.png saved
extracted text from : pdf_images/page_146.png
pdf_images/page_147.png saved
extracted text from : pdf_images/page_147.png
pdf_images/page_148.png saved
extracted text from : pdf_images/page_148.png
pdf_images/page_149.png saved
extracted text from : pdf_images/page_149.png
pdf_images/page_150.png saved
extracted text from : pdf_images/page_150.png
pdf_images/page_151.png saved
extracted text from : pdf_images/page_151.png
pdf_images/page_152.png saved
extracted text from : pdf_images/page_152.png
pdf_images/page_153.png saved
extracted text from : pdf_images/page_153.png
pdf_images/page_154.png saved
extracted text from : pdf_images/page_154.png
pdf_images/page_155.png saved
extracted text from : pdf_images/page_155.png
pdf_images/page_156.png saved
extracted text from : pdf_images/page_156.png
pdf_images/page_157.png saved
extracted text from : pdf_images/page_157.png
pdf_images/page_158.png saved
extracted text from : pdf_images/page_158.png
pdf_images/page_159.png saved
extracted text from : pdf_images/page_159.png
pdf_images/page_160.png saved
extracted text from : pdf_images/page_160.png
pdf_images/page_161.png saved
extracted text from : pdf_images/page_161.png
pdf_images/page_162.png saved
extracted text from : pdf_images/page_162.png
pdf_images/page_163.png saved
extracted text from : pdf_images/page_163.png
pdf_images/page_164.png saved
extracted text from : pdf_images/page_164.png
pdf_images/page_165.png saved
extracted text from : pdf_images/page_165.png
pdf_images/page_166.png saved
extracted text from : pdf_images/page_166.png
pdf_images/page_167.png saved
extracted text from : pdf_images/page_167.png
pdf_images/page_168.png saved
extracted text from : pdf_images/page_168.png
pdf_images/page_169.png saved
extracted text from : pdf_images/page_169.png
pdf_images/page_170.png saved
extracted text from : pdf_images/page_170.png
pdf_images/page_171.png saved
extracted text from : pdf_images/page_171.png
pdf_images/page_172.png saved
extracted text from : pdf_images/page_172.png
pdf_images/page_173.png saved
extracted text from : pdf_images/page_173.png
pdf_images/page_174.png saved
extracted text from : pdf_images/page_174.png
pdf_images/page_175.png saved
extracted text from : pdf_images/page_175.png
pdf_images/page_176.png saved
extracted text from : pdf_images/page_176.png
pdf_images/page_177.png saved
extracted text from : pdf_images/page_177.png
pdf_images/page_178.png saved
extracted text from : pdf_images/page_178.png
pdf_images/page_179.png saved
extracted text from : pdf_images/page_179.png
pdf_images/page_180.png saved
extracted text from : pdf_images/page_180.png
pdf_images/page_181.png saved
extracted text from : pdf_images/page_181.png
pdf_images/page_182.png saved
extracted text from : pdf_images/page_182.png
pdf_images/page_183.png saved
extracted text from : pdf_images/page_183.png
pdf_images/page_184.png saved
extracted text from : pdf_images/page_184.png
pdf_images/page_185.png saved
extracted text from : pdf_images/page_185.png
pdf_images/page_186.png saved
extracted text from : pdf_images/page_186.png
pdf_images/page_187.png saved
extracted text from : pdf_images/page_187.png
pdf_images/page_188.png saved
extracted text from : pdf_images/page_188.png
pdf_images/page_189.png saved
extracted text from : pdf_images/page_189.png
pdf_images/page_190.png saved
extracted text from : pdf_images/page_190.png
pdf_images/page_191.png saved
extracted text from : pdf_images/page_191.png
pdf_images/page_192.png saved
extracted text from : pdf_images/page_192.png
pdf_images/page_193.png saved
extracted text from : pdf_images/page_193.png
pdf_images/page_194.png saved
extracted text from : pdf_images/page_194.png
pdf_images/page_195.png saved
extracted text from : pdf_images/page_195.png
pdf_images/page_196.png saved
extracted text from : pdf_images/page_196.png
pdf_images/page_197.png saved
extracted text from : pdf_images/page_197.png
pdf_images/page_198.png saved
extracted text from : pdf_images/page_198.png
pdf_images/page_199.png saved
extracted text from : pdf_images/page_199.png
pdf_images/page_200.png saved
extracted text from : pdf_images/page_200.png
pdf_images/page_201.png saved
extracted text from : pdf_images/page_201.png
pdf_images/page_202.png saved
extracted text from : pdf_images/page_202.png
pdf_images/page_203.png saved
extracted text from : pdf_images/page_203.png
pdf_images/page_204.png saved
extracted text from : pdf_images/page_204.png
pdf_images/page_205.png saved
extracted text from : pdf_images/page_205.png
pdf_images/page_206.png saved
extracted text from : pdf_images/page_206.png
pdf_images/page_207.png saved
extracted text from : pdf_images/page_207.png
pdf_images/page_208.png saved
extracted text from : pdf_images/page_208.png
pdf_images/page_209.png saved
extracted text from : pdf_images/page_209.png
pdf_images/page_210.png saved
extracted text from : pdf_images/page_210.png
pdf_images/page_211.png saved
extracted text from : pdf_images/page_211.png
pdf_images/page_212.png saved
extracted text from : pdf_images/page_212.png
pdf_images/page_213.png saved
extracted text from : pdf_images/page_213.png
pdf_images/page_214.png saved
extracted text from : pdf_images/page_214.png
pdf_images/page_215.png saved
extracted text from : pdf_images/page_215.png
pdf_images/page_216.png saved
extracted text from : pdf_images/page_216.png
pdf_images/page_217.png saved
extracted text from : pdf_images/page_217.png
pdf_images/page_218.png saved
extracted text from : pdf_images/page_218.png
pdf_images/page_219.png saved
extracted text from : pdf_images/page_219.png
pdf_images/page_220.png saved
extracted text from : pdf_images/page_220.png
pdf_images/page_221.png saved
extracted text from : pdf_images/page_221.png
pdf_images/page_222.png saved
extracted text from : pdf_images/page_222.png
pdf_images/page_223.png saved
extracted text from : pdf_images/page_223.png
pdf_images/page_224.png saved
extracted text from : pdf_images/page_224.png
eaz
bytes

MASTER

MICROSERVICES

WITH SPRINGBOOT, DOCKER, KUBERNETES


COURSE AGENDA

Building microservices ; ;
Welcome to the . . How do we right size
business logic using

world of . our microservices &
. . Spring Boot . . ;
Microservices identify boundaries

How to containerize
our microservices using
Docker

COURSE AGENDA

Configurations Service Discovery & Building an edge

Management in Service Registration in server for ELAS LE SES SS
microservices using microservices using microservices using AGE UE. =f
Spring Cloud Config Eureka Spring Cloud Gateway Resiliency4J

patterns


Observability and
monitoring of
microservices using
Grafana,
Prometheus etc.

COURSE AGENDA

Securing
microservices using
OAuth2/OpenID,
Spring Security

Event Driven
microservices using
RabbitMQ, Spring
Cloud Functions &
Stream

Event Driven
microservices using
Kafka,Spring Cloud
Functions & Stream

Container
Orchestration using
Kubernetes

COURSE AGENDA

Deep dive on Helm Deploying
(kubernetes package microservices into
manager) cloud kubernetes
cluster

Many best practices,
techiniques followed
by real time
microservice
developers

€az
WHAT WE ARE GOING TO BUILD IN THIS COURSE ? bytes

External Traffic enterin

into microservice networ

using other APIs or Client
Applications

Gatew oe

API Gateway or Edge
Opent a4 & KeyCloak

Server built using
Spring Cloud Gateway

spring Cloud Eureka
. rver will act asa
service discovery &

registration agent

Sync Communication
Async Communication
<-----5>

REST APIs built using Spring Boot, Open

API documentation, validations etc Metrics, logs scraping
Async a hog Ma
STANDARDS / PROJECTS
= Mising Ka EVENT BROKER
15-Factor methodology Ra MQ
Spring Boot
Spring Cloud
Spring Security

Observability &
monitoring usin
Prometheus, Grafana

Stack & OpenTelemetry

d
y
j
y

COZZI ZL LD

Spring Cloud Functions 2
Spring Cloud Stream

RESILIENCE4J

Docker

Kubernetes

OAuth2

OpenID Connect

pdt ai

an Stack

elim

Kubernetes Cluster

eazy

What are Microservices ? bytes

To understand microservices, let's imagine a bank called EazyBank.

Typically, banks comprise various departments, including Accounts, Cards, and Loans.

Accounts

The Monolith

A SINGLE SERVER

Presentation Layer (UI)

Business Logic Layer

Data Access Layer

7

e

Supporting DB

eaz
bytes

Back a decade, all the applications used to be deployed as a Single unit where all functionality
deployed together inside a single server. We call this architecture
approach as Monolithic.

Pros

Simpler development and deployment for smaller teams and applications
Fewer cross-cutting concerns

Better performance due to no network latency

Cons

Difficult to adopt new technologies

Limited agility

Single code base and difficult to maintain

Not Fault tolerance

Tiny update and feature development always need a full deployment

We have various forms of Monolithic with the names like Single-Process Monolith, Modular
Monolith, Distributed Monolith

The Monolith bytes

Application/Web Server (Tomcat,
Jboss, Weblogic, Websphere)

; °Cté<“=iC~s™S™S™S™S™~™~™~—~—SOSOC

A ts Dev T
ccounts Dev leam Single Code repo

“f) © github —

Loans Dev Team

Spring MVC

Jenkins

Spring Data JPA/ORM

In a monolithic approach, developers work with a single code

base, which is then packaged as a unified unit, such as an Single EAR/WAR deployed

EAR/WAR file, and deployed onto a single web/application server.

Additionally, the entire application is supported by a single
Cards Dev Team Seat |

S&

UI/UX Dev Team Supporting DB

The SOA (Service-Oriented Architecture)

a a a a a ae |

SERVER 1

Presentation Layer (UI)

Middleware

Enterprise Service Bus
(ESB)

SERVER 2

Accounts Cards
Service Service

Loans
Service

----4-----

Supporting DB

eaz
bytes

SOA emerged as an approach to combat the challenges of large, monolithic applications. It is an
architectural style that focuses on organizing software systems as a collection of loosely
coupled, interoperable services. It provides a way to design and develop large-scale applications
by decomposing them into smaller, modular services that can be independently developed,
deployed, and managed.

Pros

Reusability of services
Better maintainability
Higher reliability
Parallel development

Cons

Complex management due to communication protocols (e.g., SOAP)
High investment costs due to vendor in middleware

Extra overload

eazy

The SOA (Service-Oriented Architecture) bytes

rT nl rr — roe

$ ____ © github — =a:

|
UI/UX Dev Team UI Code repo Jenkins —_——_—- = = ——_— — =

Enterprise Service

Bus

Accounts Dev Team

O © github —

Loans Dev Team

App/Web Server

ree eee eS Se

REST/SOAP Services

Jenkins

Backend Services repo

I l
I I
I l
=
\ I

. ’ | have observed numerous instances of SOA where teams aimed to create .
. . : Supporting DB
smaller services, yet they remained tightly coupled to a shared database and

required deploying everything as a cohesive unit. While they followed a service- S

Cards Dev Team

oriented approach, it cannot be classified as microservices.

The GREAT MICROSERVICES

a a a a a ae |

SERVER 1

Presentation Layer (UI)

l

l

I

l

l

l
Multiple Microservices deployed in

separate servers/containers I

l

l

l

l

l

l

l

Multiple Supporting DBs. Typically
each DB for a service

eaz
bytes

Microservices are independently releasable services that are modeled around a business
domain. A service encapsulates functionality and makes it accessible to other services via
networks—you construct a more complex system from these building blocks. One microservice
might represent Accounts, another Cards, and yet another Loans, but together they might

constitute an entire bank system.

Pros

Easy to develop, test, and deploy
Increased agility

Ability to scale horizontally

Parallel development

Modeled Around a Business Domain

Cons

Complexity
Infrastructure overhead
Security concerns

If there's one crucial takeaway from this course
and the concept of microservices, it is to
prioritize the independent deployability of your
microservices. Develop the habit of deploying

and releasing changes to a single microservice in
production without requiring the deployment of
other components. By doing so, numerous
benefits will naturally emerge.


€az

The GREAT MICROSERVICES bytes

Invokes all the backend

5S —— & github

UI Web App ™ : logic through REST
5 . API calls/Async calls
UI/UX Dev Team UI Code repo Jenkins :
: >
e : 9
6 = github —— &.F
Accounts Dev Team Jenkins Accounts microservice in 3

Accounts Code repo .
Docker container

0)

Loans Dev Team

—. github

Loans Code repo

Loans microservice in
Docker container

gq sueoy

ad spiez

Oe

> = github

Cards Dev Team

Cards microservice in

Cards Code repo Jenkins v
Docker container

MONOLITHIC vs SOA vs MICROSERVICES bytes

SOA MICROSERVICES
MONOLITHIC csr kK “= 7 ee ee =
pat ttttcc = | SERVER 1 I I SERVER 1 |
I I , I |
I I I Presentation Layer (UI!) I Presentation Layer (UI)
I Presentation Layer (UI) I I
I I I | ]
I Middleware
|
I Business Logic Layer I I Enterprise Service Bus I ) ]
| | (ESB) | Multiple Microservices deployed in
] I separate servers/containers I
server? | | im ccm!
I Data Access Layer I I I | aw Ew Gal |
Accounts Cards |
1 | | i ce
| | | I nw Ew Ga \
L a | ] Loans | I ain =" =" !
== oo I Service I ' yo] Pr] Gal !
I = = = = | I I l

Multiple Supporting DBs. Typically
Supporting DB each DB for a service

Supporting DB

MONOLITHIC vs SOA vs MICROSERVICES bytes

MONOLITHIC MICROSERVICES

SINGLE UNIT COARSE-GRAINED FINE-GRAINED


FEATURES MONOLITHIC MICROSERVICES
<*
Agility
Te ~\
Usability TT
Te =

Security Concerns & Performance

DEFINITION OF MICROSERVICE ? bytes

“Microservices is an approach to developing a
single application as a suite of small services, each
running in its own process and communicating
with lightweight mechanisms, built around
business capabilities and independently
deployable by fully automated deployment
machinery.”

- From Article by James Lewis and Martin Fowler’s


€az

TY

when considering a web application, the traditional approach involves packaging it
as a WAR or EAR file. These archive formats are commonly used to bundle Java
applications, which are then deployed to web servers like Tomcat or application
servers like WildFly.

Do you think the same approach works for building microservices ? Off course not,
because Organizations may need to build 100s of microservices. Building,
packaging, and deploying all the microservices using traditional methods can be an

extremely challenging and practically impossible task.

How to overcome this challenge ?

The clue is Spring Boot


WHY SPRING BOOT FOR MICROSERVICES? nae

bytes
WHY SPRING BOOT IS THE BEST FRAMEWORK TO BUILD MICROSERVICE

Spring Boot is a framework that simplifies the development and deployment of Java applications, including microservices. With Spring Boot,
you can build self-contained, executable JAR files instead of the traditional WAR or EAR files. These JAR files contain all the dependencies and
configurations required to run the microservice. This approach eliminates the need for external web servers or application servers.

We can quickly bootstrap a
Inbuilt support of production- fa project and start
ready features such as metrics, coding with range of starter
health checks, and dependencies that provide pre-
externalized configuration configured settings for various
components such as databases,
queues etc


€az

WHY SPRING BOOT FOR MICROSERVICES ? bytes

WHY SPRING BOOT IS THE BEST FRAMEWORK TO BUILD MICROSERVICE

In the traditional approach, applications are typically packaged as WARs and rely on the presence of a server in the execution environment for
running. However, in the micro services paradigm, applications are packaged as self-contained JARs, also called fat-JARs or uber-JARs, since they
contain the application itself, the dependencies, and the embedded server.

self-contained JARs

Microservice 1 Microservice 2
(JAR) (JAR)

Application 1 Application 2
(WAR) (WAR)

Server

(Tomcat, Jetty, Netty, Undertow)

WARs with external server JARs with embedded server
(Traditional approach) (Modern approach)

Implementing REST Services bytes

REST (Representational state transfer) services are one of the most often encountered ways to implement communication between two web
apps. REST offers access to functionality the server exposes through endpoints a client can call.

Below are the different use cases where REST services are being used most frequently these days,

Communication

eer REST Communication
+: usin

2B <—=> using REST

Mobile App Backend Server Backend Server |

Backend Server 2

Communication

<P> using REST ale

Web App built using Backend Server
Angular, React JS etc.


eazy

Typically REST services are built to expose
the business functionality and support
CRUD operations on the storage system.
Attached are the standars that we need to
follow while building REST services,

. a . . Document REST Services
Proper input validation & exception Handling

: ; |. With the help of standards like Open API Specification,
Make sure all the REST services perform input validations, Swagger, make sure to document your REST APIs. This helps

handle the runtime and business exceptions propertly. In all Your client, third party developers to understand your services
kind of scenarios, REST services should send a meaningful clearly.

response to the clients


eazy

DT Ovata Transfer Object) patte rn bytes

The Data Transfer Object (DTO) pattern is a design pattern that allows you to transfer data between different parts of your application. DTOs are
simple objects that contain only data, and they do not contain any business logic. This makes them ideal for transferring data between different layers

of your application, such as the presentation layer and the data access layer.
DB Entity

Customer

DTO Class

Client applications - Name

- Email
. - MobileNum
CustomerDetailsDTO

Name

Email

MobileNum

Account Number Accounts

Account Type
Branch Address - Account Number

- Account Type
- Branch Address

HTML page

Here are some of the benefits of using the DTO pattern:

Reduces network traffic: DTOs can be used to batch up multiple pieces of data into a single object, which can reduce the number of network requests
that need to be made. This can improve performance and reduce the load on your servers.

Encapsulates serialization: DTOs can be used to encapsulate the serialization logic for transferring data over the wire. This makes it easier to change
the serialization format in the future, without having to make changes to the rest of your application.

Decouples layers: DTOs can be used to decouple the presentation layer from the data access layer. This makes it easier to change the presentation
layer without having to change the data access layer.

Different Annotations & Classes that supports building REST
services

@ResponseBody -— can be used on top of a method to build a
Rest API when we are using @Controller on top of a Java class

@ControllerAdvice — is used to mark the class as a REST controller
ResponseEntity<T> - Allow developers to send response body, advice. Along with @ExceptionHandler, this can be used to handle
status, and headers on the HTTP response. exceptions globally inside app. We have another annotation

@RestControllerAdvice which is same as

@ControllerAdvice + @ResponseBody

RequestEntity<T> - Allow developers to receive the request body, @RequestHeader & @RequestBody — is used to receive the

header in a HTTP request. request body and header individually.

eaz
bytes

Build DB related logic,

entities & DTOs
We created required DB tables
schema, established connection
details with H2 DB, created JPA
related entities, repositories. Post
that using DTO pattern guidelines,
we built DTO classes and mapper
logic inside all the microservices

Summary of the steps Followed to build microservices

Build Global Exception
handling logic

We built global exception handling
logic using annotations like
@ControllerAdvice &
@ExceptionHandler. Also created
custom business exceptions like
CustomerAlreadyExistsException

eaz
bytes

Perform auditing using
Spring Data JPA

With the help of annotations like
@CreatedDate, @CreatedBy,
@LastModifiedDate,
@LastModifiedBy, @EntityListeners
& @EnableJpaAuditing, we
implemented logic to populate audit
columns in DB.

Build empty Spring
Boot applications

First we created empty Spring
Boot applications with the
required starter dependencies
related to web, actuator, JPA,
devtools, validations, H2 DB,
Lombok, spring doc open API
etc.

03 Build business logic

Inside all the microservices, we
created REST APIs supporting CRUD
operations with the help of various
annotations like @PostMapping,
@GetMapping, @PutMapping,
@DeleteMapping etc.

Perform data validations
on the input

Perform validations on the input data
using annotations present inside the
jakarta.validation package. These
annotations are like @NotEmpty,
@Size, @Email, @Pattern,
@vValidated, @Valid etc.

Se @eeeeseoeooees é
7 Documenting REST APIs

With the help of OpenAPI
specifications, Swagger, Spring Doc
library, we documented our REST
APIs. In the same process, we used
annotations like @Schema, @Tag,
@Operation, @ApiResponse etc.


TY

HOW TO RIGHT SIZE & IDENTIFY SERVICE BOUNDARIES OF eazy
MICROSERVICES ?

bytes

One of the most challenging aspects of building a successful microservices system is the identification
of proper microservice boundaries and defining the size of each microservice. Below are the most
common followed approaches in the industry,

Domain-Driven Sizing Event Storming Sizing
Since many of our modifications or
enhancements driven by the business
needs, we can size/define boundaries of
our microservices that are closely aligned
with Domain-Driven design & Business
capabilities. But this process takes lot of

Conducting an interactive fun session
among various stake holder to identify the
list of important events in the system like
‘Completed Payment’, ‘Search for a
Product’ etc. Based on the events we can
identify ‘Commands’, ‘Reactions’ and can
time and need good domain knowledge. try to group them to a domain-driven
services.

Reference : https://www.lucidchart.com/blog/ddd-event-storming

eaz
RIGHT SIZING & IDENTIFYING SERVICE BOUNDARIES bytes

Now, let's take an example of a bank application that needs to be migrated or built based on a microservices architecture
and attempt to determine the appropriate sizing for the services.

Saving Account Trading Account
Saving Account & Trading Account
Saving Account Trading Account Ce» Cb SD
Ce > m Debit Card Credit Card
Cards Loans
Cards & Loans
Home Loan Vehicle Loan Personal Loan

.@

NOT CORRECT SIZING AS WE CAN SEE INDEPENDENT
MODULES LIKE CARDS & LOANS CLUBBED TOGETHER

CECBCS>

NOT CORRECT SIZING AS WE CAN SEE TOO MANY
SERVICES UNDER LOANS & CARDS

THIS MIGHT BE THE MOST REASONABLE CORRECT SIZING
AS WE CAN SEE ALL INDEPENDENT MODULES HAVE
SEPARATE SERVICE MAINTAINING LOOSELY COUPLED &
HIGHLY COHESIVE

MONOLOTHIC TO MICROSERVICES bytes

MIGRATION USECASE

Now let’s take a scenario where an E-Commerce startup is following monolithic architecture and try to understand what’s the challenges
with it

eee i ee ee eS

DATABASE

CLEINT APPS

Modules

Identity Catalog

Invoices

Marketing

WEB APP |———>

l
l
l
I
l
RELATIONAL DATABASE
l
!
l
l
l

MONOLOTHIC TO MICROSERVICES bytes

MIGRATION USECASE

Problem that E-Commerce team is facing due to traditional monolithic design

Initial Days

¢ tis straightforward to build, test, deploy, troubleshoot and scale during the launch and when the team size is less

Later after few days the app/site is a super hit and started evolving a lot. Now team has below problems,

¢ The app has become so overwhelmingly complicated that no single person understands it.
¢ You fear making changes - each change has unintended and costly side effects.

* New features/fixes become tricky, time-consuming, and expensive to implement.

* Each release as small as possible and requires a full deployment of the entire application.
* One unstable component can crash the entire system.

* New technologies and frameworks aren't an option.

¢ It's difficult to maintain small isolated teams and implement agile delivery methodologies.


MONOLOTHIC TO MICROSERVICES bytes

MIGRATION USECASE

So the Ecommerce company decided and adopted the below cloud-native design by leveraging Microservices architecture
to make their life easy and less risk with the continuous changes.

IDENTITY MICROSERVICE
RDBMS «&

|

|

|

|

|

I DOCKER HOST
|

|

CATALOG MICROSERVICE

; Servs RDBMS + — —
|

ORDER MICROSERVICE

3 @ mongops --

INVOICES MICROSERVICE
> 9 mongoDB —

>

AVM3I1V9 IdV

SALES MICROSERVICE c=  REDIS
<i — EC

WEBSITE WITH
ANGULAR/REACT etc.

MARKETING MICROSERVICE _ [= _—sREDIS -
CYS > m= CACHE “&


DEPLOYMENT, PORTABILITY & SCALABILITY OF el
ytes

MICROSERVICES

PORTABILITY

How do we move our

100s of microservices
DEPLOYEMENT across environments SCALABILITY

with less effort,

. . How do we scale our

How do we deploy all configurations & cost? io based
applications based on

the tiny 100s of PP
; ; . the demand on the fly
microservices with less are Fort &
effort & cost? wien muinimenmene

cost?

TY

Docker is an open source
platform that “provides the ability
to package and run an application

in a loosely isolated environment
called a container”

To overcome the above challenges, we should containerize our microservices. Why?
Containers offer a self-contained and isolated environment for applications, including all

necessary dependencies. By containerizing an application, it becomes portable and can run
seamlessly in any cloud environment. Containers enable unified management of applications

regardless of the language or framework used.


WHAT ARE CONTAINERS & HOW THEY ARE DIFFERENT FROM VMs ?

VIRTUAL MACHINES
VM1 VM2 VvM3

Accounts Loans Cards
Service Service Service

Bins/libs Bins/libs Bins/libs

Guest OS Guest OS Guest OS

Hypervisor

Server (Physical Hardware)

|

CONTAINERS

Accounts Cards
Service Service
Bins/libs Bins/libs

Container Engine (pocker)

Loans
Service

Bins/libs

Host OS

Server (Physical Hardware)

Main differences between virtual machines and containers. Containers don’t need the Guest Os nor the
hypervisor to assign resources; instead, they use the container engine.

eaz
bytes

WHAT ARE

; ?
What is software containerization ? What is Docker ?

Docker is an open-source platform that enables developers to
automate the deployment, scaling, and management of
applications using containerization. Containers are lightweight,
isolated environments that encapsulate an application along
with its dependencies, libraries, and runtime components.

Software containerization is an OS virtualization method that is
used to deploy and run containers without using a virtual
machine (VM). Containers can run on physical hardware, in the
cloud, VMs, and across multiple OSs.


eazy

WHAT EXACTLY HAPPENS IN CONTAINERIZATION ? bytes

Containers are based on the concept of operating system (OS) virtualization, where multiple containers can run on the same physical or virtual
machine, sharing the same OS kernel. This differs from traditional virtualization, where each virtual machine (VM) runs a separate OS instance.

In containerization, Linux features such as namespaces and cgroups play a crucial role in providing isolation and resource management. Here's a
brief explanation of these concepts:

© NAMESPACES © CONTROL GROUPS

Linux namespaces allow for the creation of isolated cgroups provide resource management and

environments within the operating system. Each
container has its own set of namespaces, including
process, network, mount, IPC _ (interprocess
communication), and user namespaces. These
namespaces ensure that processes within a
container are only aware of and can interact with
resources within their specific namespace, providing
a level of isolation.

allocation capabilities for containers. They allow
administrators to control and limit the resources
(such as CPU, memory, disk I/O, and network
bandwidth) that containers can consume. By using
cgroups, container runtimes can enforce resource
restrictions and prevent one container from
monopolizing system resources, ensuring fair
allocation among containers.

Here you may have a question. If containerization works based on linux concepts like kernel, namespaces, cgroups etc. then how is

Docker supposed to work on a macOS or Windows machine. Let’s try to understand the same in next slide....

HOW DOES DOCKER WORKS ON MAC & WINDOWS OS ?

eaz
bytes

leazybytes@Eazys—MBP ~ % docker version

Client:

Cloud integration: v1.@.31

Version:

API version:
Go version:

Git commit:

Built:

23.0.5
1.42
go1.19.8
bc4487a
Wed Apr 26 16:12:52 2023

d
OS/Arch: darwin/arm64
Ontext: detau

Server: Docker Desktop 4.19.@ (106363)

Engine:
Version:

API version:
Go version:
Git commit:

OS/Arch: linux/arm64

containerd:
Version:
GitCommit:

runc:
Version:
GitCommit:

docker-init:
Version:
GitCommit:

23.0.5

1.42 (minimum version 1.12)
go1.19.8

94d3ad6

214 2023

2806fc1057397dbaeefbeaGe4e17bddfbd388F38

Pedio
v1.1.5-@-gf19387a

@.19.0
de4@ad@

eazy

DOCKER ARCHITECTURE ? bytes
DOCKER CLIENT DOCKET HOST/SERVER DOCKER REGISTRY
i = = = = = = = = = = = = = = = I moO EEE |
I l I I
12! |
Docker Remote Docker daemon | Docker Hub |
API
[ l I I
I | I l
[ l I I
We can issue commands to I | | The docker images can be |
Docker Daemon using either } Container 1 | 1 maintained and pulled from
Docker CLI or APIs | eae App | I the docker hub or private l
[ iace of Ape | [ iace of Ape | App | 3 | container registries. |
Container 2
 , ee » !
I MySQL I I |
Docker CLI Container | ' Private Registry
[ I
I l I I
I i I I
& —_ — — — — — — — — — — — — — — =| L i I I I I I | ull
1 e Instruction from Docker Client to Server to run a container 3 e Docker server pulls the image from registry into local

2 e Docker server finds the image in registry if not found locally 4. Docker server creates a running container from the image

GENERATE DOCKER IMAGES

To generare docker images from our existing microservices, we will explore the
below three different commonly used approaches. We can choose one of them for
the rest of the course

Dockerfile -> accounts

We need to write a dockerfile with the list of instructions which
can be passed to Docker server to generate a docker image

based on the given instructions

Buildpacks -> loans

Buildpacks (https://buildpacks.io), a project initiated by Heroku & Pivotal
and now hosted by the CNCF. It simplifies containerization

since with it, we don't need to write a low-level dockerfile.

Google Jib -> cards

Jib is an open-source Java tool maintained by Google for building Docker images
of Java applications. It simplifies containerization since with it, we don't need to

write a low-level dockerfile.


eaz
Running a Spring Boot app as a container using Dockerfile bytes

|”

“mvn clean instal

FROM openjdk:17-jdk-slim

Dockerfile

MAINTAINER eazybytes.com

. i
om Call I Att

docker build . -t eazybytes/accounts:s4 COPY target/accounts-0 -SNAPSHOT.jar accounts-0O -SNAPSHOT.jar

ENTRYPOINT [

“docker run -p 8080:8080 eazybytes/accounts:s4”


PORT MAPPING IN DOCKER

-p 8081:8080

User invokes http://localhost:8081

To access Accounts related APIs

Local Network

eaz
bytes

Docker Network

Accounts service
container
Port mappi oo j
o&% pping rog = running
Co —_—_—_—> at 8080
="

eaz
Running a Spring Boot app as a container using Buildpacks bytes

Sample pom.xml config
<build>
<plugins>

STEPS TO BE FOLLOWED

1) Add the configurations like mentioned on the right hand
side inside the pom.xml. Make sure to pass the image name
details

<plugin>
<groupld>org.springframework.boot</grouplId>

<artifactId>spring-boot-maven-plugins/artifactId>

<configuration>

<image>
2) Run the maven command S

from the location where pom.xml is present to generate the
docker image with out the need of Dockerfile

<name>eazybytes/${project.artifactId}:s4</name>
</image>
</configuration>
</plugin>
3) Execute the docker command </plugins>
. This will start </build>
the docker container based on the docker image name and

port mapping provided

Cloud Native Buildpacks offer an alternative approach to
Dockerfiles, prioritizing consistency, security, performance, and
governance. With Buildpacks, developers can automatically
generate production-ready OCI images from their application
source code without the need to write a Dockerfile.


eaz
Running a Spring Boot app as a container using Google Jib bytes

Sample pom.xml config

<build>

<plugins>

STEPS TO BE FOLLOWED

1) Add the configurations like mentioned on the right hand <plugiw

side inside the pom.xml. Make sure to pass the image name
details

<groupLd>com.google.cloud.tools</groupId>
<artifactId>jib-maven-plugin</artifactId>
<version>3.3.2</version>

<configuration>

2) Run the maven command

from the location where pom.xml is present to generate the
docker image with out the need of Dockerfile

<to>

<image>eazybytes/${project.artifactId}:s4</image>
</to>

</configuration>
3) Execute the docker command </plugin>
</plugins>

</build>

. This will start
the docker container based on the docker image name and
port mapping provided

Google Jib offer an alternative approach to Dockerfiles,
prioritizing consistency, security, performance, and governance.
With Jib, developers can automatically generate production-
ready OCI images from their application source code without the
need to write a Dockerfile and even local Docker setup.


Using

to handle multiple containers

What is a Docker Compose ?

It is a tool provided by Docker that allows you to define and
manage multi-container applications. It uses a YAML file to
describe the services, networks, and volumes required for your
application. Using it, you can easily specify the configuration and
relationships between different containers, making it simpler to
set up and manage complex application environments.

eaz
bytes

Advantages of Docker Compose ?

By using a single command, you can create and start all the
containers defined in your Docker Compose file. Docker
Compose handles the orchestration and networking aspects,
ensuring that the containers can communicate with each other
as specified in the configuration. It also provides options for
scaling services, controlling dependencies, and managing the
application lifecycle.


IMPORTANT DOCKER COMMANDS

docker images

To list all the docker images present in the
Docker server

docker image inspect [image-id]

To display detailed image information for a
given image id

docker image rm [image-id]

To remove one or more images for a given
image ids

docker build . -t [image-name]

To generate a docker image based ona
Dockerfile

docker run -p [hostport]:[containerport]
[image_name]

To start a docker container based on a given
image

5) 8) (@ &)

docker ps

To show all running containers

docker ps -a

To show all containers including running
and stopped

docker container start [container-id]

To start one or more stopped containers

docker container pause [container-id]

To pause all processes within one or more
containers

docker container unpause [container-id]

To resume/unpause all processes within
one or more containers

BE) BBE)

eaz
bytes

docker container stop [container-id]

To stop one or more running containers

docker container kill [container-id]

To kill one or more running containers
instantly

docker container restart [container-id]

To restart one or more containers

docker container inspect [container-id]

To inspect all the details for a given
container id

docker container logs [container-id]

To fetch the logs of a given container id

IMPORTANT DOCKER COMMANDS

88) BS

docker container logs -f [container-id]

To follow log output of a given container id

docker rm [container-id]

To remove one or more containers based on
container ids

docker container prune

To remove all stopped containers

docker image push [container_registry/
username:tag]

To push an image from a container registry

docker image pull [container_registry/
username:tag]

To pull an image from a container registry

2) ww) ®)

docker image prune

To remove all unused images

docker container stats

To show all containers statistics like CPU,
memory, I/O usage

Docker system prune

Remove stopped containers, dangling
images, and unused networks, volumes, and
cache

docker rmi [image-id]

To remove one or more images based on
image ids

docker login —u [username]

To login in to docker hub container registry

82 Be &

eaz
bytes

docker logout

To login out from docker hub container
registry

docker history [image-name]

Displays the intermediate layers and commands
that were executed when building the image

docker exec -it [container-id] sh

To open a shell inside a running container
and execute commands

docker compose up

To create and start containers based on
given docker compose file

docker compose down

To stop and remove containers for services
defined in the Compose file

eaz
What are cloud native applications ? Befe

The layman definition

Cloud-native applications are software applications designed specifically to leverage cloud
computing principles and take full advantage of cloud-native technologies and services. These
applications are built and optimized to run in cloud environments, utilizing the scalability,
elasticity, and flexibility offered by the cloud.

The Cloud Native Computing Foundation (CNCF) definition

Cloud native technologies empower organizations to build and run scalable applications in
modern, dynamic environments such as public, private, and hybrid clouds. Containers, service
meshes, microservices, immutable infrastructure, and declarative APIs exemplify this
approach.

These techniques enable loosely coupled systems that are resilient, manageable, and
observable. Combined with robust automation, they allow engineers to make high-impact
changes frequently and predictably with minimal toil.


Important characteristics of cloud-native applications

eaz
bytes

Microservices

Often built using a microservices architecture,
where the application is broken down into
smaller, loosely coupled services that can be

developed, deployed, and scaled independently

Containers

Typically packaged and deployed using
containers, such as Docker containers.
Containers provide a lightweight and
consistent environment for running
applications, making them highly portable
across different cloud platforms and

infrastructure

Resilience & Fault Tolerance

Designed to be resilient in the face of failures. They

Scalability &
Elasticity

Designed to scale horizontally, allowing
them to handle increased loads by
adding more instances of services. They
can also automatically scale up or down
based on demand, thanks to cloud-
native orchestration platforms like

Kubernetes

DevOps Practices

Embrace DevOps principles, promoting collaboration
between development and operations teams. They
often incorporate continuous integration, continuous
delivery, and automated deployment pipelines to
streamline the software development and

deployment processes.

utilize techniques such as distributed architecture,
load balancing, and automated failure recovery to
ensure high availability

and fault tolerance.

Cloud-Native Services

Take advantage of cloud-native services provided by
the cloud platform, such as managed databases,
messaging queues, caching systems, and identity
services. This allows developers to focus more on

application logic and less on managing infrastructure

components.

DIFFERENCE B/W CLOUD-NATIVE & TRADITIONAL APPS bytes

CLOUD NATIVE TRADITIONAL ENTERPRISE
APPLICATIONS APPLICATIONS
Predictable Behavior

OS abstraction

Right-sized capacity & Independent

Continuous delivery

Rapid recovery & Automated scalability


eaz
Development Principles of Cloud Native: 12 Factors & Beyond bytes

How to get succeeded in building Cloud Native Apps & what are the

guiding principles that can be considered for the same ?

The engineering team at Heroku cloud platform introduced the 12-Factor methodology, a set
of development principles aimed at guiding the design and construction of cloud-native applications.
These principles are the result of their expertise and provide valuable insights for building web
applications with specific characteristics:

1) Cloud Platform Deployment: Applications designed to be seamlessly deployed on various cloud
platforms.

2) Scalability as a Core Attribute: Architectures that inherently support scalability.

3) System Portability: Applications that can run across different systems and environments.

4) Enabling Continuous Deployment and Agility: Facilitating rapid and agile development cycles.

These principles were developed to assist developers in building effective cloud-native applications,
emphasizing the key factors that should be considered for optimal outcomes.

Subsequently, Kevin Hoffman expanded upon the original factors and introduced additional ones in his
book, “Beyond the Twelve-Factor App" This revised approach, referred to as the 15-Factor
methodology, refreshing the content of the original principles and incorporates three new factors.


15-Factor methodology

One codebase, one application

API first

Dependency management

Design, build, release, run

Configuration, credentials & code
Logs
Disposability

Backing services

8)(2) (2B 2B HVE

HHPOGHHYGVs

Environment parity

Administrative processes

Port binding

Stateless processes

Concurrency

Telemetry

Authentication &
authorization


° ° €aZz
15-Factor methodology - One codebase, one application bytes

The 15-Factor methodology ensures a one-to-one correspondence between an application and its codebase, meaning each application has a dedicated
codebase. Shared code is managed separately as a library, allowing it to be utilized as a dependency or as a standalone service, serving as a backing
service for other applications. It is possible to track each codebase in its own repository, providing flexibility and organization.

In this methodology, a deployment refers to an operational instance of the application. Multiple deployments can exist across different environments, all
leveraging the same application artifact. It is unnecessary to rebuild the codebase for each environment-specific deployment. Instead, any factors that
vary between deployments, such as configuration settings, should be maintained externally from the application codebase.

mpocccccccc

Development

at

Testing

Single Codebase

Production

Environments

15-Factor methodology - API first

In a cloud-native ecosystem, a typical setup consists of various services that
interact through APIs. Adopting an API-first approach during the design
phase of a cloud-native application encourages a mindset aligned with
distributed systems and promotes the division of work among multiple
teams. Designing the API as a priority allows other teams to build their

solutions based on that API when using the application as a backing service

This upfront design of the API contract results in more reliable and
testable integration with other systems as part of the deployment
pipeline. Moreover, internal modifications to the API implementation
can be made without impacting other applications or teams that rely
on it

eaz
bytes


eazy
15-Factor methodology - Dependency management bytes

It is crucial to explicitly declare all dependencies of an application in a manifest and ensure that they are accessible to the dependency manager, which
can download them from a central repository.

In the case of Java applications, we are fortunate to have robust tools like Maven or Gradle that facilitate adherence to this principle. The application
should only have implicit dependencies on the language runtime and the dependency manager tool, while all private dependencies must be resolved
through the dependency manager itself. By following this approach, we maintain a clear and controlled dependency management process for our
application.

Sample flow when we use Maven as build tool

3) If the dependent jar/library is not in local repository, then | |

Maven Central it searches the maven central repository 1) Maven reads the pom.xml file
Repository I I
I
4) Download the Jar I
I
I
I
5) Put the downloaded Jar |
in the local repository I
-M2 Local |
Repository I 6) Copy the jar files l
2) Check if the dependent jar/library is in | |

local repository

l = = = = = = = = |

° . €aZ
15-Factor methodology - Design, build, release, run bytes

Codebase progression from design to production deployment involves below stages,

Design stage: Determine technologies, dependencies, and tools for specific

Design stage application features.

Build stage: Compile and package the codebase with dependencies, creating an
immutable artifact (build). Unique identification of the build artifact is essential.

Build stage

Release stage: Combine the build with a specific deployment configuration. Each release
is immutable and uniquely identifiable, such as through semantic versioning (e.g., 6.1.5)
or timestamp (e.g., 2023-08-15_12:01). Central repository storage facilitates easy access,
including rollbacks if needed.

Release stage

Run stage: Execute the application in the designated runtime environment using a
specific release.

Run stage

Following the 15-Factor methodology, these stages must maintain strict separation, and runtime code modifications are
prohibited to prevent mismatches with the build stage. Immutable build and release artifacts should bear unique identifiers,
ensuring reproducibility.

° ° ° €aZ
15-Factor methodology - Configuration, credentials & code bytes

According to the 15-Factor methodology, configuration encompasses all elements prone to change between deployments. It emphasizes the ability to
modify application configuration independently, without code changes or the need to rebuild the application.

Configuration may include resource handles for backing services (e.g., databases, messaging systems), credentials for accessing third-party APIs, and
feature flags. It is essential to evaluate whether any confidential or environment-specific information would be at risk if the codebase were exposed
publicly. This assessment ensures proper externalization of configuration.

To comply with this principle, configuration should not be embedded within the code or tracked in the same codebase, except for default configuration,
which can be bundled with the application. Other configurations can still be managed using separate files, but they should be stored in a distinct
repository.

The methodology recommends utilizing environment variables to store configuration. This enables deploying the same application in different
environments while adapting its behavior based on the specific environment's configuration.

Configurations Environments

rrr rer CO

(RRR RRR RRR RRR REESE SE | 3

— oo ood

TPP CCPC) 4 Dev Config

1
l
l
© github seseeeeeespirsttttteeeeceeeeeeeeseah Testing Config pen
!
:

Single Codebase

a a

TOP PPPCCP Pe

I

|
ceeeeeeeeeccenecccesescss Dy Prod Config

I

eazy
15-Factor methodology - Logs bytes

In a cloud-native application, log routing and storage are not the application's concern. Instead, applications should direct their logs to
the standard output, treating them as sequentially ordered events based on time. The responsibility of log storage and rotation is now
shifted to an external tool, known as a log aggregator. This tool retrieves, gathers, and provides access to the logs for inspection
purposes.

Account microservice Ce» eceeseeee| LOBE foenenen
: Log
Loans microservice onmenere ee Se Aggregator
" tool
Cards microservice Ce) E-—~

15-Factor methodology - Disposa bility

eaz
bytes

In a traditional environment, ensuring the continuous operation of applications is a top priority, striving to prevent any terminations. However, in a cloud
environment, such meticulous attention is not necessary. Applications in the cloud are considered ephemeral, meaning that if a failure occurs and the

application becomes unresponsive, it can be terminated and replaced with a new instance. Similarly, during high-load periods, additional instances of the
application can be spun up to handle the increased workload. This concept is referred to as application disposability, where applications can be started or

stopped as needed.

OUL
OU

To effectively manage application instances in this dynamic
environment, it is crucial to design them for quick startup when new
instances are required and for graceful shutdown when they are no
longer needed. A fast startup enables system elasticity, ensuring
robustness and resilience. Without fast startup capabilities,
performance and availability issues may arise.

A graceful shutdown involves the application, upon receiving a
termination signal, ceasing to accept new requests, completing any
ongoing ones, and then exiting. This process is straightforward for web
processes. However, for worker processes or other types, it involves
returning any pending jobs to the work queue before exiting.

Docker containers along with an orchestrator like Kubernetes inherently satisfy
this requirement.

. . eazy
15-Factor methodology - Backing services bytes

Backing services refer to external resources that an application relies on to provide its functionality. These resources can include databases,
message brokers, caching systems, SMTP servers, FTP servers, or RESTful web services. By treating these services as attached resources, you can
modify or replace them without needing to make changes to the application code.

Consider the usage of databases throughout the software development life cycle. Typically,
different databases are used in different stages such as development, testing, and production. By
treating the database as an attached resource, you can easily switch to a different service
depending on the environment. This attachment is achieved through resource binding, which
involves providing necessary information like a URL, username, and password for connecting to
the database.

In the below example, we can see that a local DB can be swapped easily to a third-party DB like
AWS DB with out any code changes,

<= . . URL
Se aoe Microservice seceeeeressp AWSS3
=

Local DB


. . eazy
15-Factor methodology - Environment parity bytes

Environment parity aims to minimize differences between various environments & avoiding costly shortcuts. Here, the adoption of
containers can greatly contribute by promoting the same execution environment.

There are three gaps that this factor addresses:

encourages automation and continuous deployment to reduce the time between code development and

x Time gap: The time it takes for a code change to be deployed can be significant. The methodology
production deployment.

People gap: Developers create applications, while operators handle their deployment in production. To
le ¢ bridge this gap, a DevOps culture promotes collaboration between developers and operators, fostering the
"you build it, you run it" philosophy.

Tools gap: Handling of backing services differs across environments. For instance, developers might use
the H2 database locally but PostgreSQL in production. To achieve environment parity, it is recommended to
use the same type and version of backing services across all environments.

eazy

15-Factor methodology - Administrative processes bytes

fOr

Management tasks required to support applications, such as
database migrations, batch jobs, or maintenance tasks, should
be treated as isolated processes. Similar to application
processes, the code for these administrative tasks should be
version controlled, packaged alongside the application, and
executed within the same environment.

It is advisable to consider administrative tasks as independent
microservices that are executed once and then discarded, or as
functions configured within a stateless platform to respond to
specific events. Alternatively, they can be integrated directly
into the application, activated by calling a designated endpoint.

- oo. eazy
15-Factor methodology - Port binding bytes

Cloud native applications, adhering to the 15-Factor methodology, should be self-contained and expose their services through port
binding. In production environments, routing services may be employed to translate requests from public endpoints to the internally

port-bound services.

An application is considered self-contained when it doesn't rely on an
external server within the execution environment. For instance, a Java web
application might typically run within a server container like Tomcat, Jetty, or
Undertow. In contrast, a cloud native application does not depend on the
presence of a Tomcat server in the environment; it manages the server as a
dependency within itself. For example, Spring Boot enables the usage of an

4 embedded server, where the application incorporates the server instead of
relying on its availability in the execution environment. Consequently, each
yd application is mapped to its own server, diverging from the traditional

approach of deploying multiple applications on a single server.

fo

The services offered by the application are then exposed through port
binding. For instance, a web application binds its HTTP services to a specific
port and can potentially serve as a backing service for another application.
This is a common practice within cloud native systems.

eazy

15-Factor methodology - Stateless processes bytes

QB
© &

On @

Cloud native applications are often developed with high scalability in mind.
One of the key principles to achieve scalability is designing applications as
stateless processes and adopting a share-nothing architecture. This means
that no state should be shared among different instances of the application.
It is important to evaluate whether any data would be lost if an instance of
the application is destroyed and recreated. If data loss would occur, then the
application is not truly stateless.

However, it's important to note that some form of state management is
necessary for applications to be functional. To address this, we design
applications to be stateless and delegate the handling and storage of state to
specific stateful services, such as data stores. In other words, a stateless
application relies on a separate backing service to manage and store the
required state, while the application itself remains stateless. This approach
allows for better scalability and flexibility while ensuring that necessary state
is still maintained and accessible when needed.

15-Factor methodology - CONCU rrency

Scalability is not solely achieved by creating stateless applications.
While statelessness is important, scalability also requires the ability
to serve a larger number of users. This means that applications
should support concurrent processing to handle multiple users
simultaneously.

According to the 15-Factor methodology, processes play a crucial role
in application design. These processes should be horizontally scalable,
distributing the workload across multiple processes on different
machines. This concurrency is only feasible when applications are
stateless. In Java Virtual Machine (JVM) applications, concurrency is
typically managed through the use of multiple threads, which are
available from thread pools.

Processes can be categorized based on their respective types. For
instance, there are web processes responsible for handling HTTP
requests, as well as worker processes that execute scheduled
background jobs. By classifying processes and optimizing their
concurrency, applications can effectively scale and handle increased
workloads.

eaz
bytes

Vertical Scalability

Virtual Machine

Virtual Machine

2 GB RAM
2 CPU

Before

Horizontal Scalability

Virtual Machine Virtual Machine Virtual Machine

2 GB RAM 2 GB RAM 2GB RAM
2 CPU 2 CPU 2 CPU
—S— —SS——

Before After


eazy
15-Factor methodology - Telemetry bytes

Observability is a fundamental characteristic of cloud native applications. With the
inherent complexity of managing a distributed system in the cloud, it becomes essential
to have access to accurate and comprehensive data from each component of the system.
This data enables remote monitoring of the system's behavior and facilitates effective
management of its intricacies. Telemetry data, such as logs, metrics, traces, health
[ /
Gap

status, and events, plays a vital role in providing this visibility.

In Kevin Hoffman's analogy, he emphasizes the significance of telemetry by comparing
applications to space probes. Just like telemetry is crucial for monitoring and controlling
space probes remotely, the same concept applies to applications. To effectively monitor
and control applications remotely, you need various types of telemetry data.

Consider the kind of telemetry that would be necessary to ensure remote monitoring and
control of your applications. This includes information such as detailed logs for
troubleshooting, metrics to measure performance, traces to understand request flows,
health status to assess system well-being, and events to capture significant occurrences. By
gathering and utilizing these types of telemetry data, you can gain valuable insights into
your applications and make informed decisions to manage them effectively from a remote
location.

. s s z e€aZz
15-Factor methodology - Authentication and authorization bytes

Security is a critical aspect of a software system, yet it often doesn't receive
the necessary emphasis it deserves. To uphold a zero-trust approach, it is
essential to ensure the security of every interaction within the system,
encompassing architectural and infrastructural levels. While security involves
more than just authentication and authorization, these aspects serve as a
solid starting point.

Authentication enables us to track and verify the identity of users accessing the
application. By authenticating users, we can then proceed to evaluate their
permissions and determine if they have the necessary authorization to perform
specific actions. Implementing identity and access management standards can
greatly enhance security. Notable examples include OAuth 2.1 and OpenID
Connect, which we will explore in this course.


CONFIGURATION MANAGEMENT IN MICROSERVICES bytes

NJECT CONFIGs/PROPERTIES

How do we inject
configurations/properties that
SEPARATION OF microservice needed during MAINTAIN
CONFIGs/PROPERTIES start up of the service CONFIGs/PROPERTIES

How do we separate the
configurations/properties from
the microservices so that same

Docker image can be deployed in
multiple envs.

TY

How do we maintain
configurations/properties in a
centralized repository along
with versioning of them

There are multiple solutions available in Spring Boot ecosystem to handle this challenge. Below are the solutions.
Let’s try to identify which one suites for microservices

1) Configuring Spring Boot with properties and profiles
2) Applying external configuration with Spring Boot
3) Implementing a configuration server with Spring Cloud Config Server


HOW CONFIGURATIONs HANDLED IN TRADITIONAL APPs & bytes
MICROSERVICES

Traditional applications were typically bundled together with their source code and various configuration files that contained environment-specific
data. This meant that updating the configuration required rebuilding the entire application, or creating separate builds for each environment. As a
result, there was no guarantee that the application would behave consistently across different environments, leading to potential issues when moving
from staging to production.

According to the 15-Factor methodology, configuration encompasses any element likely to change between deployments, such as credentials, resource
handles, and service URLs. Cloud native applications address this challenge by maintaining the immutability of the application artifact across
environments. Regardless of the deployment environment, the application build remains unchanged. In cloud native applications, each deployment
involves combining the build with specific configuration data. This allows the same build to be deployed to multiple environments while
accommodating different configuration requirements, as shown below,

CONFIGs ENVs

Compiled & packaged as a
build which is going to

common for all

environments

One App Codebase

eueuuuuse Development Config nueeuuues Development Env

TPP)

PT QA Config BEE Eeeeee

eC

BEEBE BEES Prod Config BEBE BEES


HOW

CONFIGURATIONSw

ORK IN SPRINGBOOT ?

Spring Boot lets you externalize your configuration so
that you can work with the same application code in
different environments. You can use a variety of

external configuration sources, include Java properties
files, YAML files, environment variables, and
command-line arguments.

eaz
bytes

By default, Spring Boot look for the configurations or
properties inside application.properties/yaml present in
the classpath location. But we can have other property files
as well and make SpringBoot to read from them.

Spring Boot uses a very particular order that is designed to
allow sensible overriding of values. Properties are
considered in the following order (with values from lower
items overriding earlier ones):

Properties present inside files like application.properties
OS Environmental variables

Java System properties (System.getProperties())

JNDI attributes from java:comp/env

ServletContext init parameters

ServletConfig init parameters

Command line arguments

HOW TO READ PROPERTIES IN SPRINGBOOT APPs bytes

In Spring Boot, there are multiple approaches to reading properties. Below are the most commonly used approaches,

Using @Value
Annotation

@)

Using
Environment

@)

Using
@ConfigurationProperties

Recommended approach as
it avoids hard coding the

The @ConfigurationProperties annotation enables
binding of entire groups of properties to a bean. You
define a configuration class with annotated fields

The Environment interface provides methods to
access properties from the application's environment.

You can use the @Value annotation to directly
inject property values into your beans. This

approach is suitable for injecting individual
properties into specific fields.For example:

@Value("${property.name}")
private String propertyValue;

You can autowire the Environment bean and use its
methods to retrieve property values. This approach is
more flexible and allows accessing properties
programmatically. For example:

@Autowired

private Environment environment;

public void getProperty() {
String propertyValue =

environment.getProperty("property.name");

matching the properties, and Spring Boot
automatically maps the properties to the
corresponding fields.

@ConfigurationProperties(" prefix")
public class MyConfig {
private String property;

// getters and setters

}

In this case, properties with the prefix "prefix" will be
mapped to the fields of the MyConfig class.

Spring provides a great tool for grouping
configuration properties into so-called
profiles(dev, qa, prod) allowing us to activate a
bunch of configurations based on the active
profile.

Profiles are perfect for setting up our
application for different environments, but
they’re also being used in another use cases
like Bean creation based on a profile etc.

So basically a profile can influence the
application properties loaded and beans
which are loaded into the Spring context.

eaz
bytes

The default profile is always active. Spring Boot loads all properties in
application.properties into the default profile.

We can create another profiles by creating property files like below,

We can activate a specific profile using spring.profiles.active property
like below,

An important point to consider is that once an application is built
and packaged, it should not be modified. If any configuration
changes are required, such as updating credentials or database
handles, they should be made externally.

, . . ° ° €az
How to externalize configurations using command-line arguments ? bytes

Spring Boot automatically converts command-line arguments into key/value pairs
and adds them to the Environment object. In a production application, this becomes

the property source with the highest precedence. You can customize the application
configuration by specifying command-line arguments when running the JAR you built
earlier.

java -jar accounts-service-0.0.1-SNAPSHOT. jar --build.version="1.1"

The command-line argument follows the same naming convention as
the corresponding Spring property, with the familiar -- prefix for CLI
arguments.


How to externalized configurations using JVM system properties ? REEL

JVM system properties, similar to command-line arguments, can override Spring
properties with a lower priority. This approach allows for externalizing the
configuration without the need to rebuild the JAR artifact. The JVM system

property follows the same naming convention as the corresponding Spring
property, prefixed with -D for JVM arguments. In the application, the message
defined as a JVM system property will be utilized, taking precedence over property

files.

java -Dbuild.version="1.2" -jar accounts-service-0.0.1-SNAPSHOT. jar

In the scenario where both a JVM system property and a command-
line argument are specified, the precedence rules dictate that Spring
will prioritize the value provided as a command-line argument. This
means that the value specified through the CLI will be utilized by the
application, taking precedence over the JVM properties.


How to externalized configurations using environment variables ? bytes

Environment variables are widely used for externalized configuration as they offer
portability across different operating systems, as they are universally supported.
Most programming languages, including Java, provide mechanisms to access

environment variables, such as the System.getenv() method. P og
To map a Spring property key to an environment variable, you need to convert all

letters to uppercase and replace any dots or dashes with underscores. Spring Boot will

handle this mapping correctly internally. For example, an environment variable named

BUILD_VERSION will be recognized as the property build.version. This feature is known

as relaxed binding.

Windows

env:BUILD_VERSION="1.3"; java -jar accounts-service-0.0.1-SNAPSHOT.jar
Linux based OS
BUILD_VERSION="1.3" java -jar accounts-service-0.0.1-SNAPSHOT.jar

eaz
Drawbacks of externalized configurations using SpringBoot alone bytes

CLI arguments, JVM properties, and environment variables are effective ways to externalize configuration and maintain
the immutability of the application build. However, using these approaches often involves executing separate commands
and manually setting up the application, which can introduce potential errors during deployment.

Given that configuration data evolves and requires changes, similar to application code, what strategies should be
employed to store, track revisions and audit the configuration used in a release?

In scenarios where environment variables lack granular access control features, how can you effectively control access to
configuration data?

When the number of application instances grows, handling configuration in a
) (4) distributed manner for each instance becomes challenging. How can such challenges be
overcome?

(s) Considering that neither Spring Boot properties nor environment variables support
configuration encryption, how should secrets be managed securely?

(6) After modifying configuration data, how can you ensure that the application can read
it at runtime without necessitating a complete restart?


eazy

Spring Cloud Config bytes

A centralized configuration server with Spring Cloud Config can overcome all the drawbacks that we discussed in the previous slide. Spring Cloud
Config provides server and client-side support for externalized configuration in a distributed system. With the Config Server you have a central place
to manage external properties for applications across all environments.

Centralized configuration revolves around two core elements:

¢ Adata store designed to handle configuration data, ensuring durability, version management, and potentially access control.
¢ Aserver that oversees the configuration data within the data store, facilitating its management and distribution to multiple applications.

Microservices act as Config clients & load
configurations during startup by
connecting to Configuration service

Spring Cloud Config Server load all
the configurations by connecting to
central repository

Central repositories where
properties get stored

I | ccctetsseseey | I — !
I | | I ) ae Database I
I J ceeccccccccnse p> | is | & github ]
I I I
I I I
I I I 7 File System/ |
ile System

Caz

WHAT IS SPRING CLOUD? bytes

USING SPRING CLOUD FOR MICROSERVICES DEVELOPMENT

Spring Cloud provides frameworks for developers to quickly build some of the common patterns of Microservices

Makes sure that all calls to your microservices go Load balancing efficiently distributes

through a single “front door” before the network traffic to multiple backend servers

targeted service is invoked & the same will be or server pool

ROUTING & LOAD BALANCING

traced.
TRACING

New services will be registered & Provides features related to token-

SERVICE

. REGISTRATION &
through a logical name rather than DISCOVERY

SPRING CLOUD based security in Spring Boot
SECURITY applications/Microservices

\

later consumers can invoke them

physical location

Ensures that no matter how
many microservice instances

you bring up; they’! always

have the same configuration.

SPRING CLOUD
CONFIG

Incorporate components that helps in

understanding the complex

DISTRIBUTED
TRACING & interactions between services and
MESSAGING asynchronous communication,

allowing for scalable and resilient

systems.

Refresh configurations at runtime using /refresh path bytes

What occurs when new updates are committed to the Git repository supporting the Config Service? In a typical Spring Boot application, modifying a
property would require a restart. However, Spring Cloud Config introduces the capability to dynamically refresh the configuration in client applications
during runtime. When a change is pushed to the configuration repository, all integrated applications connected to the config server can be notified,
prompting them to reload the relevant portions affected by the configuration modification.

Let’s see an approach for refreshing the configuration, which involves sending a specific POST request to a running instance of the microservice. This
request will initiate the reloading of the modified configuration data, enabling a hot reload of the application. Below are the steps to follow,

Add actuator dependency in the Config Client services: Add Spring Boot Actuator dependency inside pom.xml of the individual
microservices like accounts, loans, cards to expose the /refresh endpoint

Enable /refresh API : The Spring Boot Actuator library provides a configuration endpoint called "/actuator/refresh" that can trigger a refresh
event. By default, this endpoint is not exposed, so you need to explicitly enable it in the application.yml file using the below config,

management:
endpoints:
web:
exposure:
include: refresh

Refresh configurations at runtime using /refresh path bytes

Reload of the new configurations data inside the
microservice with out restart of the app

Load latest configuration data related
to accounts microservice

Return latest config data related
to accounts microservice
(2) Return latest config data related (4) Pull latest changes from remote
to accounts microservice GitHub config repo

Invokes /actuator/refresh using HTTP
POST method

Accounts
microservice

e
Push new configuration data into
© @) Config repo SA gith U b
OOO, _

You invoked the refresh mechanism on Accounts Service, and it worked fine, since it was just one application with 1 instance. How about in
production where there may be multiple services ?If a project has many microservices, then team may prefer to have an automated and
efficient method for refreshing configuration instead of manually triggering each application instance. Let’s evaluate other options that we have

Config Repo

€az

Refresh configurations at runtime using Spring Cloud Bus bytes

Spring Cloud Bus, available at https://spring.io/projects/spring-cloud-bus, facilitates seamless communication between all connected application

instances by establishing a convenient event broadcasting channel. It offers an implementation for AMQP brokers, such as RabbitMQ, and Kafka,
enabling efficient communication across the application ecosystem.

Below are the steps to follow,

@)
@)

OxC

Add actuator dependency in the Config Server & Client services: Add Spring Boot Actuator dependency inside pom.xml of the individual
microservices like accounts, loans and cards to expose the /busrefresh endpoint

Enable / busrefresh API: The Spring Boot Actuator library provides a configuration endpoint called "/actuator/busrefresh" that can trigger a
refresh event. By default, this endpoint is not exposed, so you need to explicitly enable it in the application.yml file using the below config,

management:
endpoints:
web:
exposure:
include: busrefresh

Add Spring Cloud Bus dependency in the Config Server & Client services: Add Spring Cloud Bus dependency (spring-cloud-starter-bus-amqp)
inside pom.xml of the individual microservices like accounts, loans, cards and Config server

Set up a RabbitMQ: Using Docker, setup RabbitMQ service. If the service is not started with default values, then configure the rabbitmq
connection details in the application.yml file of all the individual microservices and Config server

Refresh configurations at runtime using Spring Cloud Bus bytes

Reload of the new configurations data from Config Server
inside all the microservices with out restart

Loans
microservice

Accounts Message
microservice Broker

Cards

microservice Trigger a config change event &
initiate refresh on all the

subscribed nodes

Invoke /actuator/busrefresh on

any of the microservice
instance & config service wil

load the latest config data

00
IX

github

Config Repo

@ Push new configuration data into
Config repo
gO

Though this approach reduce manual work to a great extent, but still there is a single manual step involved which is invoking the
/actuator/busrefresh on any of the microservice instance. Let’s see how we can avoid and completely automate the process.

Refresh configurations at runtime using Spring Cloud Bus & Spring Cloud bytes

Config Monitor

Spring Cloud Config offers the Monitor library, which enables the triggering of configuration change events in the Config Service. By exposing the
/monitor endpoint, it facilitates the propagation of these events to all listening applications via the Bus. The Monitor library allows push notifications
from popular code repository providers such as GitHub, GitLab, and Bitbucket. You can configure webhooks in these services to automatically send a
POST request to the Config Service after each new push to the configuration repository. Below are the steps to follow,

Add actuator dependency in the Config Server & Client services: Add Spring Boot Actuator dependency inside pom.xml of the individual
microservices like accounts, loans, cards and Config server to expose the /busrefresh endpoint

Enable / busrefresh API: The Spring Boot Actuator library provides a configuration endpoint called "/actuator/busrefresh" that can trigger a
refresh event. By default, this endpoint is not exposed, so you need to explicitly enable it in the application.yml file using the below config,
management:
endpoints:
web:
exposure:
include: busrefresh

Add Spring Cloud Bus dependency in the Config Server & Client services: Add Spring Cloud Bus dependency (spring-cloud-starter-bus-amaqp)
inside pom.xml of the individual microservices like accounts, loans, cards and Config server

Add Spring Cloud Config monitor dependency in the Config Server : Add Spring Cloud Config monitor dependency (spring-cloud-config-
monitor) inside pom.xml of Config server and this exposes /monitor endpoint

Set up a RabbitMQ: Using Docker, setup RabbitMQ service. If the service is not started with default values, then configure the rabbitmq
connection details in the application.yml file of all the individual microservices and Config server

Set up a WebHook in GitHub: Set up a webhook to automatically send a POST request to Config Service /monitor path after each new push
to the config repo.

OOOO OO

Refresh configurations at runtime using Spring Cloud Bus & Spring Cloud eazy
Config Monitor bytes

Reload of the new configurations data from Config Server
inside all the microservices with out restart

Loans
microservice

Accounts Message
microservice Broker

Cards

microservice Trigger a config change event &
initiate refresh on all the ebhook Invokes /moniotr on
subscribed nodes config server

Push new configuration data into °
Config repo git U
SS

Config Repo

In this solution, there is no manual step involved and everything is automated.

Liveness and Readiness probes

A liveness probe sends a signal that the container or application is either alive

(passing) or dead (failing). If the container is alive, then no action is required because the
current state is good. If the container is dead, then an attempt should be made to heal the
application by restarting it.

In simple words, liveness answers a true-or-false question: "Is this container alive?"

A read ness probe used to know whether the container or

app being probed is ready to start receiving network traffic. If your
container enters a state where it is still alive but cannot handle
incoming network traffic (a common scenario during startup), you
want the readiness probe to fail. So that, traffic will not be sent to a
container which isn't ready for it.

If someone prematurely send network traffic to the container, it could
cause the load balancer (or router) to return a 502 error to the client
and terminate the request. The client would get a "connection
refused" error message.

In simple words, readiness answers a true-or-false question: "Is this
container ready to receive network traffic ?"

Inside Spring Boot apps, actuator gathers the “Liveness” and “Readiness” information from the
ApplicationAvailability interface and uses that information in dedicated health indicators:
LivenessStateHealthIndicator and ReadinessStateHealthIndicator. These indicators are shown on
the global health endpoint ("/actuator/health"). They are also exposed as separate HTTP Probes
by using health groups: "/actuator/health/liveness" and "/actuator/health/readiness"

SERVICE DISCOVERY & REGISTRATION IN MICROSERVICES bytes

AOW DO NEW SERVICE INSTANCE
ENTER INTO THE NETWORK?

If an microservice instance fails, new

instances will be brought online to
HOW DO SERVICES LOCATE ensure constant availability. This

EACH OTHER INSIDE A means that the IP addresses of the
NETWORK? instances can be constantly changing
Each instance of a microservice 50 Les
exposes a remote API with it's own wes IES new Sees can load balance b/w the multiple
host and port. how do other Sea SS MDE Rare Se microservice instances especially a
microservices & clients know microservice is invoking another
about these dynamic endpoint microservice? How do a specific
URLs to invoke them. So where is service information shared across
my service? the network?

OAD BALANCE, INFO SHARING
B/W MICROSERVICE
INSTANCES

How do we make sure to properly

These challenges in microservices can be solved using below concepts or solutions,

1) Service discovery
2) Service registration
3) Load balancing


€aZz
How service communication happens in Traditional apps ? bytes

Inside web network, When a service/app want to communicate with another service/app, it must be given the necessary information to locate it,
such as an IP address or a DNS name. Let's examine the scenario of two services, Accounts and Loans. If there was only a single instance of Loans
microservice, below figure illustrates how the communication between the two applications would occur.

Upstream Service Downstream Service

Internal communication between microservices using
Accounts hostname, DNS or IP address
———_ hh

No Service Discovery or Load Balancing involved

Loans
microservice

microservice

127.54.37.2
. es s es . . 3 . s
Loans microservice will be a backing service with respect to Accounts microservice

When there is only one instance of the Loans microservice running, managing the DNS name and its corresponding IP address mapping is
straightforward. However, in a cloud environment, it is common to deploy multiple instances of a service, with each instance having its own unique IP
address.

To address this challenge, one approach is to update DNS records with multiple IP addresses and rely on round-robin name resolution. This method
directs requests to one of the IP addresses assigned to the service replicas in a rotating manner. However, this approach may not be suitable for
microservices, as containers or services change frequently. This rapid change makes it difficult to maintain accurate DNS records and ensure efficient
communication between microservices.

Unlike physical machines or long-running virtual machines, cloud-based service instances have shorter lifespans. These instances are designed to be
disposable and can be terminated or replaced for various reasons, such as unresponsiveness. Furthermore, auto-scaling capabilities can be enabled to
automatically adjust the number of application instances based on the workload.

€aZ
How Traditional LoadBalancers works ? bytes

Clients like other services uses generic DNS along with the service specific path to invoke a specific service

I
l
I
| = = Ll = = = = = Ll = Ll = = = = = = Ll = Ll = Ll = Ll = Ll = J

services.eazybank.com/accounts services.eazybank.com/cards services.eazybank.com/loans

Secondary Load Balancer

<<

S Routing tables om Health checks

il

Accounts Service Loans Service Cards Service
= = = = = = = = = = = = = = = = = = = = = = = = = = |

Traditional Service location resolution architecture using DNS & a load balancer

€aZ
Limitations with Traditional LoadBalancers ? bytes

With traditional approach each instance of a service used to be deployed in one or more application
servers. The number of these application servers was often static and even in the case of restoration it
would be restored to the same state with the same IP and other configurations.

While this type of model works well with monolithic and SOA based applications with a relatively small

number of services running on a group of static servers, it doesn’t work well for cloud-based microservice
applications for the following reasons,

| | Limited horizontal scalability & licenses costs

| Single point of failure & Centralized chokepoints ASL.

The biggest challenge with traditional
load balancers is that some one has
| | Manually managed to update any IPs, configurations to manually maintain the routing
tables which is an impossible task
inside the microservices network.

Because containers/services are
ephemeral in nature

_—

| | Complex in nature & not containers friendly

eazy

How to solve the problem for cloud native applications ? bytes
aS! ce For cloud native applications, service discovery is the perfect
solution. It involves tracking and storing information about all

running service instances in a service registry.

Whenever a new instance is created, it should be registered in the
registry, and when it is terminated, it should be appropriately
removed automatically.

The registry acknowledges that multiple instances of the same
application can be active simultaneously. When an application needs
to communicate with a backing service, it performs a lookup in the
registry to determine the IP address to connect to. If multiple
instances are available, a load-balancing strategy is employed to
evenly distribute the workload among them.

Client-side service discovery and server-side service discovery are distinct approaches that address
the service discovery problem in different contexts

e€aZ
How to solve the problem for cloud native applications ? bytes

In a modern microservice architecture, knowing the right network location of an application is a much
more complex problem for the clients as service instances might have dynamically assigned IP addresses.
Moreover the number instances may vary due to autoscaling and failures.

Microservices service discovery & registration is a way for applications and microservices to locate each
other on a network. This includes,

A central server (or servers) that maintain a global view of addresses

Microservices/clients that connect to the central server to register
their address when they start & ready

Vly

Microservices/clients need to send their heartbeats at regular ~ () 7

ot . . a‘
intervals to central server about their health sacle discovery & registrations =
deals with the problems about how

microservices talk to each other, i.e.
Microservices/clients that connect to the central server to deregister perform API calls.

their address when they are about to shutdown SS

€aZ
Client-side service discovery and load balancing bytes

In client-side service discovery, applications are responsible for registering themselves with a service registry during startup and unregistering
when shutting down. When an application needs to communicate with a backing service, it queries the service registry for the associated IP
address. If multiple instances of the service are available, the registry returns a list of IP addresses. The client application then selects one based
on its own defined load-balancing strategy. Below figure illustrates the workflow of this process

oans service registers with Service registry during startup
& send regular heart beats

Registry

Dude, what are address Buddy, heere are the IP address of Loans

details of loans service ? instances,
127.54.37.23 Loans
127.54.37.24 microservice

Accounts
127.54.37.2

microservice

Accounts microservice is going to invoke one
of the instance of loans based on the load

balancing strategy configured

Loans
microservice

127.54.37.2
4

Client-side service discovery and load balancing bytes

Client-side service discovery is an architectural pattern where client applications are responsible for
locating and connecting to services they depend on. In this approach, the client application communicates
directly with a service registry to discover available service instances and obtain the necessary information
to establish connections.

Here are the key aspects of client-side service discovery: on

Service Registration: Client applications register themselves with the service registry upon
startup. They provide essential information about their location, such as IP address, port, and
metadata, which helps identify and categorize the service.

queries the service registry for available instances of that service. The registry responds with
the necessary information, such as IP addresses and connection details.

Load Balancing: Client-side service discovery often involves load balancing to distribute the
workload across multiple service instances. The client application can implement a load-
balancing strategy to select a specific instance based on factors like round-robin, weighted
distribution, or latency.

| | Service Discovery: When a client application needs to communicate with a specific service, it 7.

The Spring Cloud project provides

several alternatives for incorporating
The major advantage of client-side service discovery is load balancing can be implemented - - : . .
. . . . : . client-side service discovery in our
using various algorithms, such as round-robin, weighted round-robin, least connections, or . . .
even custom algorithms. A drawback is that client service discovery assigns more Spring Boot based microservices. More

responsibility to developers. Also, it results in one more service to deploy and maintain (the details to follow...

service registry). Server-side discovery solutions solve these issues. We are going to discuss
the same when we are talking about Kubernetes TT

How Client-side service discovery works

Service discovery nodes
communicate with each
other about new services,
health of the services etc.

When a service comes online it
register its IP address with a service

discovery agent and let it know that it
is ready to take requests

Client Applications(Microservices) never worry about the direct IP details of the microservice. They will just invoke service
discovery layer with a logical service name

I

| I
Other Microservices Other Microservices

| I

| =

services.eazybank.com/accounts services.eazybank.com/cards services.eazybank.com/loans
A service actual location will
be looked up based on the
given logical name
= = = = = = = = = = = = = = = = = = = = = = = =

Service Discovery Layer
cu om cr
a <—_|_— ————_— ra aru

Service Discovery Node 1 Service Discovery Node 2 Service Discovery Node 3

Fegn iD —_ <r E> —— <n FD

L Accounts Service Loans Service Cards Service
= = = = = = = = = = = = = = = = = = = = = = = = = =

eaz
bytes

Service instances send a heartbeat to
the service discovery agent. If a service
didn’t send a heartbeat, service
discovery will remove the IP of the dead
instance from the list

How loadbalancing works in Client-side service discovery ?

When a microservice want to connect with other microservice, it will check the local cache for the service instances IPs. Load
If the client finds a service IP in the balancing also happens at the service level itself w/o depending on the Service Discovery

cache, it will directly invokes the — eee

backing service. Otherwise it goes to ]
the service discovery (ey I

eaz
bytes

Periodically the client side cache
will be refreshed with the service

discovery layer

Service Discovery Layer
oe ______, cos «__________, ams
oe —-,— _— |

Service Discovery Node 1 Service Discovery Node 2 Service Discovery Node 3

---+t--------#---------t----5

Other microservices I
| in the network I

Cards Service

Loans Service
= = = = = = = = = = = = = = = = = = = =

oo i

Service discovery nodes
communicate with each
other about new services,
health of the services etc.

Service instances send a heartbeat to
the service discovery agent. If a service
didn’t send a heartbeat, service
discovery will remove the IP of the dead
instance from the list

€aZ
Spring Cloud support for Client-side service discovery bytes

+.

| | Spring Cloud Netflix's Eureka service which will act as a service discovery agent _£’ /

Spring Cloud project makes Service Discovery & Registration setup trivial to undertake with the help of the
below components,

Spring Cloud Load Balancer library for client-side load balancing 2

| Netflix Feign client to look up for a service b/w microservices

Advantages of Service Discovery approach

includes,
Though in this course we use Eureka since it is mostly used but they are other service

registries such as etcd,Consul, and Apache Zookeeper which are also good. * No limitations on availability

* Peer to peer communication b/w Services Discovery
Though Netflix Ribbon client-side is also good and stable product, we are going to use Spring

agents
Cloud Load Balancer for client-side load balancing. This is because Ribbon has entered a * Dynamically managed IPs, configurations & Load
maintenance mode and unfortunately, it will not be developed anymore a ee

¢ Fault-tolerant & Resilient in nature

_—

eazy

Steps to build Eureka Server bytes

Below are the steps to build a Eureka Server application using Spring Cloud Netflix’s Eureka,

@)
@)

QO ©

Set up a new Spring Boot project: Start by creating a new Spring Boot project using your preferred IDE or by using Spring Initializr
(https://start.spring.io/). Include the spring-cloud-starter-netflix-eureka-server maven dependency.

Configure the properties: In the application properties or YAML file, add the following configurations,

server:
port: 8070

eureka:
instance:
hostname: localhost
client:
fetchRegistry: false
registerWithEureka: false
serviceUrl:
defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/

Add the Eureka Server annotation: In the main class of your project, annotate it with @EnableEurekaServer. This annotation configures the
application to act as a Eureka Server.

Build and run the Eureka Server: Build your project and run it as a Spring Boot application. Open a web browser and navigate to
http://localhost:8070. You should see the Eureka Server dashboard, which displays information about registered service instances.

. . . . €aZ
Steps to register a microservice as a Eureka Client bytes

Below are the steps to make a microservice application to register and act as a Eureka client,

Set up a new Spring Boot project: Start by creating a new Spring Boot project using your preferred IDE or by using Spring Initializr
(https://start.spring.io/). Include the spring-cloud-starter-netflix-eureka-client maven dependency.

(2) Configure the properties: In the application properties or YAML file, add the following configurations,

eureka:
instance:
preferlpAddress: true
client:
registerWithEureka: true
fetchRegistry: true
serviceUrl:
defaultZone: "http://localhost:8070/eureka/"

http://localhost:8070. You should see the microservice registered itself as an application and the same can be confirmed inside the Eureka
Server dashboard.

(3) Build and run the application: Build your project and run it as a Spring Boot application. Open a web browser and navigate to

EUREKA SELF-PRESERVATION TO AVOID TRAPS IN NETWORK bytes

In a distributed system using Eureka, each service instance periodically sends a heartbeat signal to the
Eureka server to indicate that it is still alive and functioning. If the Eureka server does not receive a
heartbeat from a service instance within a certain timeframe, it assumes that the instance has become
unresponsive or has crashed. In normal scenarios, this behavior helps the Eureka server maintain an up-to-
date view of the registered service instances.

However, in certain situations, network glitches or temporary system delays may cause the Eureka server
to miss a few heartbeats, leading to false expiration of service instances. This can result in unnecessary
evictions of healthy service instances from the registry, causing instability and disruption in the system.

To mitigate this issue, Eureka enters into Self-Preservation mode. When Self-Preservation mode is active, the
existing registry entries will not be removed even if it stops receiving heartbeats from some of the service
instances. This prevents the Eureka server from evicting all the instances due to temporary network glitches
or delays.

In Self-Preservation mode, the Eureka server continues to serve the registered instances to client
applications, even if it suspects that some instances are no longer available. This helps maintain the stability
and availability of the service registry, ensuring that clients can still discover and interact with the available
instances.

Self-preservation mode never expires, until and unless the down microservices are brought back or the
network glitch is resolved. This is because eureka will not expire the instances till it is above the
threshold limit.

Eureka Server will not panic when it is
not receiving heartbeats from majority of
the instances, instead it will be calm and
enters into Self-preservation mode. This

feature is a savior where the networks

glitches are common and help us to
handle false-positive alarms.

SS

eazy

EUREKA SELF-PRESERVATION TO AVOID TRAPS IN NETWORK bytes

I fees | Instance 1 -— UP |
] = ee aa tt Instance 2 - UP
Peer to peer communication Instance 3 - UP |
| —— — : Instance 4 - UP
| Eureka Server 1 Eureka Server 2 peta e te I
43 ne
eee / 1 == ee ay eee J
OS
/ 1 Sey \ . s
4 ‘ sw Heartbeat by all the ‘ %
/ Ss instances for every 30secs ‘ SK
/ ‘ s s
4 \ . ‘ s
Al \ ~, \ .
1
f mmm EO a = = = a. = = Ll = ca moO Oe an = = =. mom |
x
, é; \ r , x ~
Instance 3 Instance 4 Instance 5 |

Loans Service Instances

Healthy Microservices System with all 5 instances up before encountering network problems

eaz
EUREKA SELF-PRESERVATION TO AVOID TRAPS IN NETWORK bytes

cr |
] eee ea ee Instance 1 — UP
Peer to peer communication as Instance 2 - UP
| —— — : Instance 3 - UP
| Eureka Server 1 Eureka Server 2 I
I I I | rl 3 I I I | |
/ \ Sey
’ \ .
4 ‘ Ss Heartbeat by all the
/ Ss instances for every 30secs
! \ x
4 \ x
’ \ ‘\
\
f moo gE <= Eo ca ee ee ee ee ee |
' ¥ y ‘~
| Instance 1 Instance 2 Instance 3 Instance 4 Instance 5 J

Loans Service Instances

2 of the instances not sending heartbeat. Eureka enters self-preservation mode since it met threshold
percentage

eaz
EUREKA SELF-PRESERVATION TO AVOID TRAPS IN NETWORK bytes

cr |
] eee ea ee Instance 1 — UP
Peer to peer communication Instance 2 - UP |
| —— — : Instance 3 - UP
| Eureka Server 1 Eureka Server 2 I
4
I I I | Pas I | |
an
4
a \
4 ‘ Heartbeat by all the
hi r instances for every 30secs
4 \
4 \
/ \
U \
f moo gE \ ee ee ee ee ee ee |
| Instance 1 Instance 2 Instance 3 Instance 4 Instance 5 J

Loans Service Instances

During Self-preservation, eureka will stop expiring the instances though it is not receiving heartbeat from
instance 3

EUREKA SELF-PRESERVATION TO AVOID TRAPS IN NETWORK

eaz
bytes

Configurations which will directly or indirectly impact self-preservation behavior of eureka

v

eureka.instance.lease-renewal-interval-in-seconds = 30
Indicates the frequency the client sends heartbeats to server to indicate that it is still alive

eureka.instance.lease-expiration-duration-in-seconds = 90
Indicates the duration the server waits since it received the last heartbeat before it can evict an instance

eureka.server.eviction-interval-timer-in-ms = 60 * 1000

A scheduler(EvictionTask) is run at this frequency which will evict instances from the registry if the lease of the instances are

expired as configured by lease-expiration-duration-in-seconds. It will also check whether the system has reached self-preservation
mode (by comparing actual and expected heartbeats) before evicting.

eureka.server.renewal-percent-threshold = 0.85
This value is used to calculate the expected % of heartbeats per minute eureka is expecting.

eureka.server.renewal-threshold-update-interval-ms = 15 * 60 * 1000
A scheduler is run at this frequency which calculates the expected heartbeats per minute

eureka.server.enable-self-preservation = true
By default self-preservation mode is enabled but if you need to disable it you can change it to ‘false’


ROUTING, CROSS CUTTING CONCERNS IN MICROSERVICES bytes

HOW DO WE HANDLE CROSS
CUTTING CONCERNS?

In a distributed microservices
architecture, how do we make sure to
HOW DO WE MAINTAIN A have a consistently enforced cross

SINGLE ENTRYPOINT INTO cutting concerns like
MICROSERVICES NETWORK ? logging, auditing, tracing and securi
across multiple microservices

How do we build a single gatekeeper
for all the inbound traffic to our
microservices. This way the client
doesn’t need to keep track of the
different services involved in a
transaction, simplifying the client’s
logic

These challenges in microservices can be solved using a

Edge server


eazy

ROUTING, CROSS CUTTING CONCERNS IN MICROSERVICES bytes

Loans
microservice

cards
microservice

In a scenario where multiple clients directly
connect with various services, several
challenges arise. For instance, clients must
be aware of the URLs of all the services, and
enforcing common requirements such as
security, auditing, logging, and routing
becomes a repetitive task across all services.
To address these challenges, it becomes
necessary to establish a single gateway as
the entry point to the microservices
network.

SS

ROUTING,

CROSS CUTTING CONCERNS IN MICROSERVICES

Edge server/
API Gateway

Loans
microservice

cards
microservice

eaz
bytes

Edge servers are applications positioned at edge
of a system, responsible for implementing
functionalities such as API gateways and
handling cross-cutting concerns. By utilizing edge
servers, it becomes possible to prevent cascading
failures when invoking downstream services,
allowing for the specification of retries and
timeouts for all internal service calls.
Additionally, these servers enable control over
ingress traffic, empowering the enforcement of
quota policies. Furthermore, authentication and
authorization mechanisms can be implemented
at the edge, enabling the passing of tokens to
downstream services for secure communication
and access control.

—_—_———

e€aZ
Few important tasks that API Gateway does bytes

Clients
Web Mobile APIs

API Gateway

Request ae AuthN Rate Exception Circuit
Validation Exclude list ie Limit Handling Breaker
(2) AuthZ
Protocol Modify Service Dynamic Logging
Conversion 8 Req/Res 7 Discovery (6) Routing Monitoring Cache

- = Observability .
> Microservices Tools like Z redis

Grafana


Spring Cloud Gateway

Spring Cloud Gateway streamlines the creation of edge services by emphasizing ease and efficiency.
Moreover, due to its utilization of a reactive framework, it can seamlessly expand to handle the significant

workload that typically arises at the system's edge while maintaining optimal scalability.

Here are the key aspects of Spring Cloud Gateway,

The service gateway sits as the gatekeeper for all inbound traffic to microservice calls within
our application. With a service gateway in place, our service clients never directly call the
URL of an individual service, but instead place all calls to the service gateway.

Spring Cloud Gateway is a library for building an API gateway, so it looks like any another
Spring Boot application. If you’re a Spring developer, you'll find it’s very easy to get started
with Spring Cloud Gateway with just a few lines of code.

Spring Cloud Gateway is intended to sit between a requester and a resource that’s being
requested, where it intercepts, analyzes, and modifies every request. That means you can
route requests based on their context. Did a request include a header indicating an API
version? We can route that request to the appropriately versioned backend. Does the request
require sticky sessions? The gateway can keep track of each user’s session.

Spring Cloud Gateway is the preferred API gateway compared to zuul. Because Spring Cloud
Gateway built on Spring Reactor & Spring WebFlux, provides circuit breaker integration, service
discovery with Eureka, non-blocking in nature, has a superior performance compared to that of

Zuul.

The service gateway sits between all
calls from the client to the individual
services & acts as a central Policy
Enforcement Point (PEP) like below,

* Routing (Both Static & Dynamic)
* Security (Authentication & Authorization)
* Logging, Auditing and Metrics collection

———— SS

€aZ
Spring Cloud Gateway Internal Architecture bytes

SPRING CLOUD GATEWAY MICROSERVICES

r= —_ sc=s=S=-c rer eee
I
I

I
l
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
ul

| I , I I
| Predicates Request y
I (To check if the requests rd
| fulfill a set of given condition) |
;
Pre Filters I I
I Request ] ] I
CLIENTs ] y I ]
8 I I y I
l \ I l
- Request | ]
~, ——s —
ttt: 7 Response ] Response ] I
Response ] |
I I
l l
Post Filters I I
I y I I

When the client makes a request to the Spring Cloud Gateway, the Gateway Handler Mapping first checks if the request matches a route. This matching is done using the
predicates. If it matches the predicate then the request is sent to the pre filters followed by actual microservices. The response will travel through post filters.

Steps to create Spring Cloud Gateway bytes

Below are the steps to make a microservice application to register and act as a Eureka client,

Set up a new Spring Boot project: Start by creating a new Spring Boot project using your preferred IDE or by using Spring Initializr
@) (https://start.spring.io/). Include the spring-cloud-starter-gateway, spring-cloud-starter-config & spring-cloud-starter-netflix-eureka-client

maven dependencies.

RouteLocatorBuilder

eureka:
instance:
preferlpAddress: true
client:
registerWithEureka: true
fetchRegistry: true
serviceUrl:
defaultZone: http://localhost:8070/eureka/
spring:
cloud:
gateway:
discovery:
locator:
enabled: true
lowerCaseServiceld: true

(2) Configure the properties: In the application properties or YAML file, add the following configurations. Make routing configurations using

Steps to create Spring Cloud Gateway bytes

(3) Configure the routing config: Make routing configurations using RouteLocatorBuilder like shown below,

@Bean
public RouteLocator myRoutes (RouteLocatorBuilder builder) {
return builder.routes ()
-route(p -> p
-path ("/eazybank/accounts/**")
.filters(f -> £.rewritePath ("/eazybank/accounts/ (?<segment>.*)","/${segment}")
. addResponseHeader ("X-Response-Time",new Date() .toString()))
-uri ("1b://ACCOUNTS") )
-route(p -> p
- path ("/eazybank/loans/**")
.filters(f -> £.rewritePath ("/eazybank/loans/ (?<segment>.*)","/${segment}")
. addResponseHeader ("X-Response-Time",new Date() .toString()))
-uri ("1b://LOANS") )
-route(p -> p
- path ("/eazybank/cards/**")
.filters(f -> £.rewritePath ("/eazybank/cards/ (?<segment>.*)","/${segment}")
. addResponseHeader ("X-Response-Time",new Date() .toString()))
-uri ("1b://CARDS") ) .build() ;

Build and run the application: Build your project and run it as a Spring Boot application. Invokes the APIs using http://localhost:8072 which
is the gateway path.

eaz
RESILIENCY IN MICROSERVICES bytes

HOW DO WE HANDLE
FAILURES GRACEFULLY WITH

FALLBACKS?

In a chain of multiple
microservices, how do we

HOW DO WE AVOID build a fallback mechanism if HOW TO MAKE OUR
CASCADING FAILURES? one aii meroservice is not SERVICES SELF-
working. Like returning a HEALING CAPABLE

default value or return values

One failed or slow service
should not have a ripple effect on from cache or call another .
In the cases of slow performing

the other microservices. Like in the service/DB to fetch the results
etc.

: £ multiole mi : services, how do we configure
scenarios © mu 'P © microservices timeouts, retries and give time
are communicating, we need to P .
: : for a failed services to recover
make sure that the entire chain of itself
microservices does not fail with the
failure of a single microservice

Ensuring system stability and resilience is crucial for providing a reliable service to users. One of the critical aspects in achieving a stable
and resilient system for production is managing the integration points between services over a network.

There exist various patterns for building resilient applications. In the Java ecosystem, Hystrix, a library developed by Netflix, was widely
used for implementing such patterns. However, Hystrix entered maintenance mode in 2018 and is no longer being actively developed. To
address this, Resilience4J has gained significant popularity, stepping in to fill the gap left by Hystrix. Resilience4J provides a comprehensive
set of features for building resilient applications and has become a go-to choice for Java developers.

RESILIENCY USING RESILIENCE4J bytes

Resilience4j is a lightweight fault tolerance library designed for functional programming. It offers the following patterns for
increasing fault tolerance due to network problems or failure of any of the multiple services:

Circuit breaker - Used to stop making requests when a service invoked is

failing

Fallback - Alternative paths to failing requests

Rate limit - Limits the number of calls that a service receives in a time

Bulkhead - Limits the number of outgoing concurrent requests to a service to avoid

| Retry - Used to make retries when a service has temporarily failed

overloading

TYPICAL SCENARIO IN MICROSERVICES

Edge server/

API Gateway

/fetchCustomerDetails
pp

accounts
microservice

cards
microservice

eaz
bytes

When a microservice responds slowly or fails to
function, it can lead to the depletion of resource
threads on the Edge server and intermediate
services. This, in turn, has a negative impact on
the overall performance of the microservice
network.

To handle this kind of scenarios, we can use
Circuit Breaker pattern

_———

CIRCUIT BREAKER PATTERN bytes

In an electrical system, a circuit breaker is a safety device designed to protect the electrical circuit from excessive current,
preventing damage to the circuit or potential fire hazards. It automatically interrupts the flow of electricity when it detects
a fault, such as a short circuit or overload, to ensure the safety and stability of the system.

The Circuit Breaker pattern in software development takes its inspiration from the concept of an electrical circuit breaker
found in electrical systems.

In a distributed environment, calls to remote resources and services can fail due to transient faults, such as slow network connections, timeouts, or the
resources being overcommitted or temporarily unavailable. These faults typically correct themselves after a short period of time, and a robust cloud
application should be prepared to handle them.

The Circuit Breaker pattern which inspired from electrical circuit breaker will monitor the remote calls. If the calls take too long, the circuit breaker will
intercede and kill the call. Also, the circuit breaker will monitor all calls to a remote resource, and if enough calls fail, the circuit break implementation
will pop, failing fast and preventing future calls to the failing remote resource.

The Circuit Breaker pattern also enables an application to detect whether the fault has been resolved. If the problem appears to have been fixed, the
application can try to invoke the operation.
The advantages with circuit breaker pattern are,

Vv Fail fast

Vv Fail gracefully
vY Recover seamlessly

eazy
CIRCUIT BREAKER PATTERN bytes

In Resilience4j, the circuit breaker is implemented via three states

Failure rate above
threshold

OPEN -— If Circuit breaker sees
a threshold requests are
failing, then it will OPEN the
circuit which will make
requests fail fast

CLOSED -— Initially the circuit
breaker starts with closed
status and accepts client
requests

HALF_OPEN -— Periodically Circuit

ly breaker checks if the issue is @
“e resolved by allowing few < Oe
%, Wy requests. Based on the results it ee Re
Qn, SG will either go to CLOSED or OPEN ~P
4 Y @
y%, © SF oh
Y Y

eazy
CIRCUIT BREAKER PATTERN bytes

Below are the steps to build a circuit breaker pattern using Spring Cloud Gateway filter,

G) Add maven dependency: Add spring-cloud-starter-circuitbreaker-reactor-resilience4j maven dependency inside pom.xml

Add circuit breaker filter: Inside the method where we are creating a bean of RouteLocator, add a filter of circuit breaker like highlighted
below and create a REST API handling the fallback uri /contactSupport

@Bean
public RouteLocator myRoutes (RouteLocatorBuilder builder) {
return builder.routes ()
.route(p -> p.path ("/eazybank/accounts/**")
.filters(f -> £.rewritePath ("/eazybank/accounts/ (?<segment>.*)","/${segment}")

. addResponseHeader ("X-Response-Time",new Date() .toString())
.circuitBreaker (config -> config.setName ("accountsCircuitBreaker")
.setFallbackUri ("forward: /contactSupport") ) )

.uri ("1b://ACCOUNTS") ) .build() ;

(3) Add properties: Add the below properties inside the application.yml file,

resilience4j.circuitbreaker:
configs:
default:
slidingWindowSize: 10
permittedNumberOfCallsInHalfOpenState: 2
failureRateThreshold: 50
waitDurationInOpenState: 10000

eazy
CIRCUIT BREAKER PATTERN bytes

Below are the steps to build a circuit breaker pattern using ;

G) Add spring-cloud-starter-circuitbreaker-resilience4j maven dependency inside pom.xml

t(name= , fallback = CardsFallback.class)
public interface CardsFeignClient {

ping(value = ,consumes =
public ResponseEntity<CardsDto> fetchCardDetails( tHeader(
String correlationId, @Request! n String mobileNumber);

public class CardsFallback implements CardsFeignClient{

public ResponseEntity<CardsDto> fetchCardDetails(String correlationId, String mobileNumber) {

return null;


eazy
CIRCUIT BREAKER PATTERN bytes

(3) Add properties: Add the below properties inside the application.yml file,

spring:
cloud:
openfeign:
circuitbreaker:
enabled: true
resilience4j.circuitbreaker:
configs:
default:
slidingWindowSize: 5
failureRateThreshold: 50
waitDurationInOpenState: 10000
permittedNumberOfCallsInHalfOpenState: 2

RETRY PATTERN bytes

The retry pattern will make configured multiple retry attempts when a service has temporarily failed. This pattern is very
helpful in the scenarios like network disruption where the client request may successful after a retry attempt.

Here are some key components and considerations of implementing the Retry pattern in microservices:

| Retry Logic: Determine when and how many times to retry an operation. This
can be based on factors such as error codes, exceptions, or response status.

Backoff Strategy: Define a strategy for delaying retries to avoid overwhelming
the system or exacerbating the underlying issue. This strategy can involve
gradually increasing the delay between each retry, known as exponential
backoff.

| | Circuit Breaker Integration: Consider combining the Retry pattern with the
Circuit Breaker pattern. If a certain number of retries fail consecutively, the
circuit can be opened to prevent further attempts and preserve system
resources.
| | idempotent Operations: Ensure that the retried operation is idempotent,

meaning it produces the same result regardless of how many times it is
invoked. This prevents unintended side effects or duplicate operations.

eazy
RETRY PATTERN bytes

Below are the steps to build a retry pattern using Spring Cloud Gateway filter,

G) Add Retry filter: Inside the method where we are creating a bean of RouteLocator, add a filter of retry like highlighted below,

@Bean
public RouteLocator myRoutes (RouteLocatorBuilder builder) {
return builder.routes ()
.route(p -> p.path("/eazybank/loans/**")
.filters(f -> £.rewritePath ("/eazybank/loans/ (?<segment>.*)","/${segment}")

.addResponseHeader ("X-Response-Time",new Date() . toString () )
.retry (retryConfig -> retryConfig.setRetries (3) .setMethods (HttpMethod.GET)
. setBackoff (Duration. ofMillis (100) ,Duration.ofMillis (1000) ,2,true) ) )
uri ("1b://LOANS") ) .build() ;


eazy
RETRY PATTERN bytes

Below are the steps to build a retry pattern using normal Spring Boot service,

@) Add Retry pattern annotations: Choose a method and mention retry pattern related annotation along with the below configs. Post that
create a fallback method matching the same method signature like we discussed inside the course,

@Retry (name = "getBuildiInfo", fallbackMethod = "getBuildInfoFallBack")
@GetMapping ("/build-info")
public ResponseEntity<String> getBuildiInfo() {

private ResponseEntity<String> getBuildInfoFallBack (Throwable t) {


RETRY PATTERN

(2) Add properties: Add the below properties inside the application.yml file,

resilience4j.retry:
configs:
default:
maxRetryAttempts: 3
waitDuration: 500
enableExponentialBackoff: true
exponentialBackoffMultiplier: 2
retryExceptions:

- java.util.concurrent.TimeoutException
ignoreExceptions:

- java.lang.NullPointerException

eaz
bytes

RATE LIMITTER PATTERN bytes

The Rate Limiter pattern in microservices is a design pattern that helps control and limit the rate of incoming requests to a service
or API. It is used to prevent abuse, protect system resources, and ensure fair usage of the service.

In a microservices architecture, multiple services may depend on each other and make requests to
communicate. However, unrestricted and uncontrolled requests can lead to performance degradation,
resource exhaustion, and potential denial-of-service (DoS) attacks. The Rate Limiter pattern provides a
mechanism to enforce limits on the rate of incoming requests.

Implementing the Rate Limiter pattern helps protect microservices from being overwhelmed by
excessive or malicious requests. It ensures the stability, performance, and availability of the system
while providing controlled access to resources. By enforcing rate limits, the Rate Limiter pattern
helps maintain a fair and reliable environment for both the service provider and its consumers.

When a user surpasses the permitted number of requests within a designated time frame, any
additional requests are declined with an HTTP 429 - Too Many Requests status. The specific limit is
enforced based on a chosen strategy, such as limiting requests per session, IP address, user, or tenant.
The primary objective is to maintain system availability for all users, especially during challenging
circumstances. This exemplifies the essence of resilience. Additionally, the Rate Limiter pattern proves
beneficial for providing services to users based on their subscription tiers. For instance, distinct rate
limits can be defined for basic, premium, and enterprise users.


RATE LIMITTER PATTERN

Below are the steps to build a rate limitter pattern using Spring Cloud Gateway filter,

G) Add maven dependency: Add spring-boot-starter-data-redis-reactive maven dependency inside pom.xml and make sure a redis container
started. Mention redis connection details inside the application.yml file

Add rate limitter filter: Inside the method where we are creating a bean of RouteLocator, add a filter of rate limitter like highlighted below
and creating supporting beans of RedisRateLimiter and KeyResolver

@Bean
public RouteLocator myRoutes (RouteLocatorBuilder builder) {

return builder.routes ()
.route(p -> p.path ("/eazybank/cards/**")
.filters(f£f -> £.rewritePath ("/eazybank/cards/ (?<segment>.*)","/${segment}")

. addResponseHeader ("X-Response-Time" ,new Date() .toString() )

.requestRateLimiter (config ->
config.setRateLimiter (redisRateLimiter ()) .setKeyResolver (userKeyResolver ())))

-uri("1b://CARDS") ) .build() ;
}

@Bean
public RedisRateLimiter redisRateLimiter() {

return new RedisRateLimiter(1, 1, 1);

}

@Bean
KeyResolver userKeyResolver() {
return exchange -> Mono. justOrEmpty (exchange.getRequest() .getHeaders() .getFirst("user") )

.defaultifEmpty ("anonymous") ;

eaz
bytes


eazy
RATE LIMITTER PATTERN bytes

Below are the steps to build a rate limiter pattern using normal Spring Boot service,

@) Add Rate limitter pattern annotations: Choose a method and mention rate limitter pattern related annotation along with the below configs.
Post that create a fallback method matching the same method signature like we discussed inside the course,

@RateLimiter (name = "getJavaVersion", fallbackMethod = "getJavaVersionFallback")

@GetMapping ("/java-version")
public ResponseEntity<String> getJavaVersion() {

private ResponseEntity<String> getJavaVersionFallback (Throwable t) {

(2) Add properties: Add the below properties inside the application.yml file,

resilience4j.ratelimiter:
configs:
default:
timeoutDuration: 5000
limitRefreshPeriod: 5000
limitForPeriod: 1

BULKHEAD PATTERN bytes

The Bulkhead pattern in software architecture is a design pattern that aims to improve the resilience and isolation of components
or services within a system. It draws inspiration from the concept of bulkheads in ships, which are physical partitions that prevent
the flooding of one compartment from affecting others, enhancing the overall stability and safety of the vessel.

In the context of software systems, the Bulkhead pattern is used to isolate and limit the
impact of failures or high loads in one component from spreading to other components. It
helps ensure that a failure or heavy load in one part of the system does not bring down the
entire system, enabling other components to continue functioning independently.

THWARTSHIP
BULKHE ADS

Bulkhead Pattern helps us to allocate limit the resources which can be used for specific
services. So that resource exhaustion can be reduced.

FORE AND AFT
BULKHEADS,

The Bulkhead pattern is particularly useful in systems that require high availability, fault tolerance, and
isolation between components. By compartmentalizing components and enforcing resource boundaries, the
Bulkhead pattern enhances the resilience and stability of the system, ensuring that failures or heavy loads in
one area do not bring down the entire system.

BULKHEAD PATTERN

REQUESTS

HINA

Without Bulkhead, /myCustomerDetails will start eating all
the threads, resources available which will effect the

ACCOUNTS MICROSERVICES

/myAccount

/myCustomerDetails

—
—

performance of /myAccount

REQUESTS

HIN

eaz
bytes

ACCOUNTS MICROSERVICES

/myAccount

a
—

/ myCustomerDetails

aaa

With Bulkhead, /myCustomerDetails and /myAccount will
have their own resources, threads pool defined

OBSERVABILITY AND MONITORING OF MICROSERVICES bytes

MONITORING

PERFORMANCE OF
SERVICE CALLS?

How can we track the path of a specific
DEBUGGING A PROBLEM i
Net eees Snob bi iain MONITORING SERVICES
: long it took to complete at each METRICS & HEALTH ?
How do we trace transactions across microservice?
multiple services, containers and try to find
where exactly the problem or bug is?

How can we easily and efficiently monitor
the metrics like CPU usage, JVM metrics,
etc. for all the microservices applications in
How do we combine all the logs from our network?
multiple services into a central location
where they can be indexed, searched,
filtered, and grouped to find bugs that
are contributing to a problem?

How can we monitor the status and
health of all of our microservices
applications in a single place, and create
alerts and notifications for any abnormal
behavior of the services?

Observability and monitoring solve the challenge of identifying and resolving above
problems in microservices architectures before they cause outages.

eaz
WHAT IS OBSERVABILITY ? bytes

Observability is the ability to understand the internal state of a system by observing its outputs. In
the context of microservices, observability is achieved by collecting and analyzing data from a variety
of sources, such as metrics, logs, and traces.

The three pillars of observability are: ~

ed Metrics: Metrics are quantitative measurements of the health
of a system. They can be used to track things like CPU usage, +>
memory usage, and response times.

we Logs: Logs are a record of events that occur in a system. They can
be used to track things like errors, exceptions, and other unexpected
events.

we Traces: Traces are a record of the path that a request takes ;

through a system. They can be used to track the performance of a
request and to identify bottlenecks.

By collecting and analyzing data from these three sources, you can gain a comprehensive understanding of the internal state of your microservices

architecture. This understanding can be used to identify and troubleshoot problems, improve performance, and ensure the overall health of your
system.

eaZz
WHAT IS MONITORING ? bytes

Monitoring in microservices involves checking the telemetry data available for the application and defining alerts
for known failure states. This process collects and analyzes data from a system to identify and troubleshoot
problems, as well as track the health of individual microservices and the overall health of the microservices
network.

Monitoring in microservices is important because it allows you to:

your microservices, you can identify problems before they cause outages or

K Identify and troubleshoot problems: By collecting and analyzing data from
other disruptions.

Track the health of your microservices: Monitoring can help you to track the
health of your microservices, so you can identify any microservices that are
underperforming or that are experiencing problems.

Optimize your microservices: By monitoring your microservices, you can identify
areas where you can optimize your microservices to improve performance and
reliability.

Monitoring and observability can be considered as two sides of the same coin. Both rely on the same types of telemetry data to enable insight into
software distributed systems. Those data types — metrics, traces, and logs — are often referred to as the three pillars of observability.

7 os eazy
Observability vs. Monitoring bytes

oe Monitoring Observability
Identify and troubleshoot Understand the internal state of a
Purpose
problems system

Monitoring

Data Mere ee eenenall es Metrics, traces, logs, and other
data sources
Identify problems Understand how a system works

Obse rvability In other words, monitoring is about collecting data and
observability is about understanding data.

Monitory is reacting to problems while observavility is fixing
them in real time.

; eazy
Logging bytes

N Logs are discrete records of events that happen in software applications over time. They contain a timestamp that indicates when the
event happened, as well as information about the event and its context. This information can be used to answer questions like "What
LOG happened at this time?", "Which thread was processing the event?", or "Which user/tenant was in the context?"

Logs are essential tools for troubleshooting and debugging tasks. They can be used to reconstruct what happened at a specific point in time in a single
application instance. Logs are typically categorized according to the type or severity of the event, such as trace, debug, info, warn, and error. This allows
us to log only the most severe events in production, while still giving us the chance to change the log level temporarily during debugging.

ead Logging in Monolithic Apps

In monolithic apps, all of the code is in a single codebase. This means that all of the logs are also in a single location. This makes it easy
to find and troubleshoot problems, as you only need to look in one place.

Se Logging in Microservices

Logging in microservices is complex. This is because each service has its own logs. This means that you need to look in multiple places to
find all of the logs for a particular request.

To address this challenge, microservices architectures often use centralized logging. Centralized logging collects logs from all of the
services in the architecture and stores them in a single location. This makes it easier to find and troubleshoot problems, as you only
need to look in one place

€aZ
Managing logs with Grafana, Loki & Promtail bytes

Grafana is an open-source analytics and interactive
visualization web application. It provides charts,
graphs, and alerts for the web when connected to
supported data sources. It can be easily installed
using Docker or Docker Compose.

Grafana is a popular tool for visualizing metrics, logs,
and traces from a variety of sources. It is used by
organizations of all sizes to monitor their applications
and infrastructure.

Grafana Loki is a horizontally scalable, highly available, and cost-effective log aggregation system. It is designed to be easy to use and to scale to meet the
needs of even the most demanding applications.

Promtail is a lightweight log agent that ships logs from your containers to Loki. It is easy to configure and can be used to collect logs from a wide variety
of sources.

Together, Grafana Loki and Promtail provide a powerful logging solution that can help you to understand and troubleshoot your applications.

Grafana provides visualization of the log lines captured within Loki.

FROM GRAFANAOKI3.0, Managing logs with Grafana, Loki & Alloy bytes

' Alloy replaced Promtail

‘ehent side full page load

1G:50 1E:65 ore aa) 0G oa | 15 rage nee ne. 2] Ths oo ras

Grafana Loki is a horizontally scalable, highly available, and cost-effective log aggregation system. It is designed to be easy to use and to scale
to meet the needs of even the most demanding applications.

Grafana Alloy is a lightweight log agent that ships logs From your containers to Loki. It is easy to configure and can be used to collect logs Froma
wide variety of sources.

Together, Grafana Loki and Alloy provide a powerful logging solution that can help you to understand and troubleshoot your applications.

Grafana provides visualization of the log lines captured within Loki.

Managing logs with Grafana, Loki & Promtail bytes

Edge server/
API Gateway

cards
microservice

accounts
microservice

Query & Search

Fetches logs logs

@_2==8282828282s28s221s1s=s=

s
I
I
I
I
1
I
I
I
I

Promtail
collects logs from
containers, process &
Forwards them Loki

Forward logs


Managing logs with Grafana, Loki & Alloy bytes

FROM GRAFANA LOKI 3.0,
Alloy replaced Promtail

Edge server/
API Gateway

Sec ‘a Fa na
Qu se , visualize
wheloee “ial oi woe ‘i as S

cards
microservice

accounts
microservice

Query & Search

Fetches logs logs

Qs 2 2 ew ee ee eee

&
i
l
1
i
i
i
I
i
i

Alloy

collects logs From

Forward logs Loki

log aggregation system

containers, process &
Forwards them Loki


Sample demo of logging using Grafana, Loki & promotail bytes

| fog | cloud-native applications generate logs as events and send
them to the standard output, without being concerned about

J v09s_/ , the processing or storage of those logs.

One advantage of treating logs as event streams and emitting

: them to stdout is that it decouples the application From the

: log processing infrastructure. The application can Focus on its

: 2 ~~" query and query results  olaiead core Functionality without being tied to a specific logging
an CHEpOUEE implementation or storage solution. The infrastructure, on
the other hand, can handle the collection, aggregation, and
storage of logs using appropriate tools and services.

| oan 15-Factor methodology recommends the same to treat logs
————S— as events streamed to the standard output and not concern

with how they are processed or stored.


Sample demo of logging using Grafana, Loki & Alloy bytes

Reference: https://grafana.com/docs/loki/latest/get-started/quick-star | mercaniont
FROM GRAFANA LOKI 3.0,

Alloy replaced Promtail

Application

them to the standard output, without being concerned about
the processing or storage of those logs.

cloud-native applications generate logs as events and send

One advantage of treating logs as event streams and emitting
them to stdout is that it decouples the application From the
ASgatorciiont log processing infrastructure. The application can Focus on its

core Functionality without being tied to a specific logging

implementation or storage solution. The infrastructure, on
the other hand, can handle the collection, aggregation, and
storage of logs using appropriate tools and services.

Storage

Loki

Loki

Read Path

Gateway

NGINS

15-Factor methodology recommends the same to treat logs
as events streamed to the standard output and not concern
with how they are processed or stored.

CG


Metrics & monitoring with Spring Boot Actuator, Micrometer, bytes

Prometheus & Grafana

Event logs are essential for monitoring applications, but they don't provide enough data to answer all of the questions we need to know. To answer
questions like CPU usage, memory usage, threads usage, error requests etc. & properly monitor, manage, and troubleshoot an application in production,
we need more data.

Metrics are numerical measurements of an application's performance, collected and aggregated at regular intervals. They
can be used to monitor the application's health and performance, and to set alerts or notifications when thresholds are

exceeded.
(2) MICROMETER

Micrometer automatically exposes /actuator/metrics data into something
your monitoring system can understand. All you need to do is include that

vendor-specific micrometer dependency in your application. Think SLF4J,

(4) Grafana

Grafana is a visualization tool
that can be used to create
dashboards and charts from
Prometheus data.

but for metrics.

@) ACTUATOR

Actuator is mainly used to expose operational

Prometheus

The most common format for exporting metrics is the
one used by Prometheus, which is “an open-source
health, metrics, info, dump, env, etc. It uses HTTP systems monitoring and alerting toolkit”. Just as Loki

endpoints or JMX beans to enable us to interact aggregates and stores event logs, Prometheus does the
with it same with metrics.

information about the running application —

eazy

Metrics & monitoring with Spring Boot Actuator, Micrometer, bytes

Prometheus & Grafana

Edge server/
API Gateway

cards
microservice

accounts
microservice

Microservices with the
help of actuator and
micrometer exposes
metrics information

Query metrics
from

@_2==8282828282s28s221s1s=s=

scrapes metrics
from services Prometheus

———_eeee e e e e e  ee e e e e e e e e e e e e e ee eee Metrics aggregator &
monitoring systen

4
I
I
I
I
I
I
I
I
I
I
I
I
I
I
LL.


. ae ae , eazy
Distributed tracing in microservices bytes

Event logs, health probes, and metrics offer a wealth of valuable information for deducing the internal condition of an application.
Nevertheless, these sources fail to account for the distributed nature of cloud-native applications. Given that a user request often traverses
multiple applications, we currently lack the means to effectively correlate data across application boundaries.

Distributed tracing is a technique used in microservices or cloud-native applications to understand and analyze the flow of requests as they propagate
across multiple services and components. It helps in gaining insights into how requests are processed, identifying performance bottlenecks, and
diagnosing issues in complex, distributed systems.

One possible solution to address this issue is to implement a straightforward approach where a unique identifier, known as a correlation ID, is

Eg generated for each request at the entry point of the system. This correlation ID can then be utilized in event logs and passed along to other
relevant services involved in processing the request. By leveraging this correlation ID, we can retrieve all log messages associated with a
specific transaction from multiple applications.

Me Distributed tracing encompasses three primary concepts:

Tags serve as metadata that offer supplementary details about the span context, including the request URI, the username of the authenticated
user, or the identifier for a specific tenant.

A trace denotes the collection of actions tied to a request or transaction, distinguished by a trace ID. It consists of multiple spans that span
across various services.

A span represents each individual stage of request processing, encompassing start and end timestamps, and is uniquely identified by the
combination of trace ID and span ID.

ae . . . eazy
Distributed tracing in microservices bytes

When a client request received at the edge server or the
first service inside the network, a trace ID like
29cdbe2e21bc will be generated and it is going to be
same throught the request

[gatewayserver, 29cdbe2e21bc, 259d5208]

[loans, 29cdbe2e21bc, 9e5b7f81] — logger statement 1

[loans, 29cdbe2e21bc, — logger statement 2

Edge server/ accounts
API Gateway microservice

A Span ID is going to be generated which is
specific to a service and the same will be present
in all the logs of the service in a request

29cdbe2e21bc, 61fd3a92]

cards
microservice

Apart from Trace ID and Span ID, a metadata
information can also be attached to the logs using Tags.
For example, here we added serive name which is [cards, 29cdbe2e21bc, 8f4b7a76] — logger statement 1
accounts

[cards, 29cdbe2e21bc, 8f4b7a76] — logger statement 2

Distributed tracing with OpenTelemetry, Tempo bytes

& Grafana

@) OpenTelemetry (2) Tempo

Usuing OpenTelemetry generate traces and spans Index the tracing information using Grafana Tempo.
automatically. OpenTelemetry also known as Otel for Tempo is an open-source, highly scalable, and cost-effective
short, is a vendor-neutral open-source Observability distributed tracing backend designed for observability in
framework for instrumenting, generating, collecting, cloud-native environments. It is a part of the Grafana

and exporting telemetry data such as traces, metrics, observability stack and provides a dedicated solution for

logs.

efficient storage, retrieval, and analysis of trace data.

(3) Grafana

Using Grafana, we can connect to Tempo as a
datasource and see the distributed tracing in
action with the help of visuals. We can
integrate Loki and Tempo as well, so that we
can jump to tracing details directly from logs
inside Loki

eazy

Distributed tracing with OpenTelemetry, Tempo bytés

& Grafana

Edge server/
API Gateway
cards
microservice

accounts
microservice

Query traces from

Opentelemetry java agent
JAR dynamically injects
bytecode to add trace
information & send to
Tempo

@_2==8282828282s28s221s1s=s=

1
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
LL.

Send traces
to


eazy
MICROSERVICES SECURITY bytes

AUTHENTICATION AND
AUTHORIZATION

How can our microservices
can authenticate and authorize

SECURING MICROSERVICES users and services to access them.
FROM UNAUTHORIZED ACESS Our microservices should be capable

of performing identification,
? authentication & authorization.

How to secure our microservices and
protect them from unauthorized access
by client applications or end users ?
Right now all our services doesn’t have
security and any one can invoke them to
get a reponse which have sensitive data

Using OAuth2/OpenID Connect, KeyCloak (IAM), Spring

Security we can secure the microservices and handle all the above challenges.

eazy

PROBLEM THAT OAUTH2 SOLVES bytes

Why should we use OAUTH2 framework for implementing security inside our microservices ? Why can’t we use the basic
authentication ? To answer this, first lets try to understand the basic authentication & it’s drawbacks.

jeco
. Early websites usually ask for credentials via
Login an HTML form, which the browser will send to
the server. The server authenticates the
information and writes a session value in the
cookie; as long as the session is still marked

active, user can access protected features and

Of
(2 |

resources.
Drawbacks of Basic authentication
Backend server or business logic is tightly coupled with the Basic authentication flow does not accommodate well the use
Authentication/Authorization logic. Not mobile flow/ REST ic case where users of one product or service would like to grant
APIs friendly third-party clients access to their information on the platform.

eaZz
PROBLEM THAT OAUTH2 SOLVES bytes

How come, Google let me
use the same account in all
it’s products ? Though they
are different websites/

Apps ?

Well the answer is with the
help of OAuth2. OAuth2
recommend to use a
separate Auth

server for Authentication &
Authorization


eazy
INTRODUCTION TO OAUTH2 bytes

OAuth stands for Open Authorization. It’s a free and open protocol, built on IETF standards and licenses from the Open Web Foundation.

OAuth 2.1 is a security standard where you give one application permission to access your data in another application. The steps to grant
permission, or consent, are often referred to as authorization or even delegated authorization. You authorize one application to access your data,
or use features in another application on your behalf, without giving them your password.

Below are the few advantages of OAuth2 standard,

Supports all kinds of Apps: OAuth2 supports multiple use cases addressing different device

Me capabilities. It supports server-to-server apps, browser-based apps, mobile/native apps, loT
devices and consoles/TVs. It has various authorization grant flows like Authorization Code grant,
Client Credentials Grant Type etc. to support all kinds of apps communication.

Separation of Auth logic: Inside OAuth2, we have Authorization Server which receives requests from

Eg the Client for Access Tokens and issues them upon successful authentication. This enable us to
maintain all the security logic in a single place. Regardless of how many applications an organization
has, they all can connect to Auth server to perform login operation.

All user credentials & client application credentials will be maintained in a single location which is
inside Auth Server.

se No need to share Credentials: If you plan to allow a third-party applications and services to access your resources, then there is no need
to share your credentials.

In many ways, you can think of the OAuth2 token as a “access card” at any office/hotel. These tokens provides limited access to someone,
without handing over full control in the form of the master key.

eazy

OAUTH2 TERMINOLOGY bytes

Resource owner - It is you the end user. In the scenario of Stackoverflow, the end user who want to use the
GitHub services to get his details. In other words, the end user owns the resources (email, profile), that’s why we call

him as Resource owner

Client — The website, mobile app or API will be the client as it is the one which interacts with GitHub services
on behalf of the resource owner/end user. In the scenario of Stackoverflow, the Stackoverflow website is Client

Authorization Server — This is the server which knows about resource owner. In other words, resource
owner should have an account in this server. In the scenario of Stackoverflow, the GitHub server which has
authorization logic acts as Authorization server.

Resource Server — This is the server where the resources that client want to consume are hosted. In the
scenario of Stackoverflow, the resources like User Email, Profile details are hosted inside GitHub server. So it will

act as a resource server.

ScOpes — These are the granular permissions the Client wants, such as access to data or to perform certain
actions. The Auth server can issue an access token to client with the scope of Email, READ etc.

eaz
WHAT IS OPENID CONNECT & WHY IT IS IMPORTANT ? bytes

What is OpenID Connect?

OpenID Connect is a protocol that sits on top of the OAuth 2.0 framework. While OAuth 2.0 provides authorization via an
access token containing scopes, OpenID Connect provides authentication by introducing a new ID token which contains a
new set of information and claims specifically for identity.

With the ID token, OpenID Connect brings standards around sharing identity details among the applications.

The OpenID Connect flow looks the same as OAuth. The only
OAuth 2.0 ———_ Authorization

differences are, in the initial request, a specific scope

of openid is used, and in the final exchange the client receives
both an Access Token and an ID Token.


eaz
WHAT IS OPENID CONNECT & WHY IT IS IMPORTANT ? bytes

Why is OpenID Connect important?

Identity is the key to any application. At the core of modern authorization is OAuth 2.0, but OAuth 2.0 lacks an authentication component.
Implementing OpenID Connect on top of OAuth 2.0 completes an IAM (Identity & Access Management) strategy.

As more and more applications need to connect with each other and more identities are being populated on the internet, the demand to be
able to share these identities is also increased. With OpenID connect, applications can share the identities easily and standard way.

OpenID Connect add below details to OAuth 2.0

1. OIDC standardizes the scopes to openid, profile, email,

Authorization and address.
OAuth 2.0

2. ID Token using JWT standard

3. OIDC exposes the standardized “/userinfo” endpoint.

Identity Access Management

CLIENT CREDENTIALS GRANT TYPE FLOW IN OAUTH2

CLIENT AUTH SERVER RESOURCE SERVER
-— Cry

Cx

honor = =

1 1) ~
| want to access protected resources.

Here are my client credentials. No user
involved in this

Hey Client, The credentials provided are correct.
Here is the ACCESS TOKEN to
access the protected resources

Hey Resource Server, | want to access a
protected resources. Here is the access
token issued by Auth server

Hey Client. Your token is validated 4)
successfully. Here are the resources
you requested

eaz
bytes

eaz
CLIENT CREDENTIALS GRANT TYPE FLOW IN OAUTH2 bytes

Y In the step 1, where client is making a request to Auth Server endpoint, have to send the below
important details,

client_id & client_secret — the credentials of the client to authenticate itself.
scope -— similar to authorities. Specifies level of access that client is requesting like EMAIL,

PROFILE
grant_type — With the value ‘client_credentials’ which indicates that we want to

follow client credentials grant type

Y This is the most simplest grant type flow in OAUTHZ2.

Y We use this authentication flow only if there is no user and UI involved. Like in the scenarios where
2 different applications want to share data between them using backend APIs.


SECURING GATEWAY USING CLIENT CREDENTIALS GRANT TYPE FLOW eazy

IN OAUTH2 DINGS
AUTH SERVER (3)

Diccvc.oak
Edge server validates the

access token issued by Auth
server
Client connects with Auth
/

server with client
credentials and get a access

Loans
microservice

token

Client invokes a path in

edge server using access
token issued by Auth server

If the access token valid,
edge server allows the
request to the actual
External services Accounts

\
. : \
service/API microservice /

Cards
microservice
CLIENT RESOURCE SERVER

Unsecured services deployed
behind the docker network or

Kubernetes firewall network. So
can’t be accessed directly

* When ever an external system trying to communicate with Spring
Cloud Gateway where there is no end user involved, then we need
to use the OAuth2 Client Credentials grant flow for Authentication &
Authorization.

SECURING GATEWAY USING CLIENT CREDENTIALS GRANT TYPE FLOW
IN OAUTH2

RESOURCE SERVER

External client trying to invoke /eazybank/accounts/api/**
with out any authentication

Gateway/Resource server replies, “Sorry Buddy, I can only

process the requests who provide a access token from Auth 2

server. Go and get a access token from Auth Server’.

3 External client asked KeyCloak which is a Auth Server for Access token

Auth Server laughed and replied, | can’t give access token just like that. In order to get access
token from me, you need to register with me and the same needs to be approved by my admin

CLIENT

eaz
bytes

ZiKEYCLOAK

AUTH SERVER

SECURING GATEWAY USING CLIENT CREDENTIALS GRANT TYPE FLOW eazy
IN OAUTH2 eee

RESOURCE SERVER

External client trying to invoke /eazybank/accounts/api/**
with the access token that it received during the step 6

Gateway shared the received access token
to the Auth Server to confirm whether it is

valid or not

Gateway/Resource server invokes the actual microservice
API & respond back with the successful response

Auth Server confirmed back to the
Gateway that the access token is

valid

ZiKEYCLOAK

5 External client asked KeyCloak which is a Auth Server for Access token. But this time, it passed Client ID & Client Secret
which it received during the registration process that it did offline with the admin of the Auth server

Auth Server replied, “Congratulations Buddy. You details are correct. 6
Here is your access token. All the Best”

CLIENT AUTH SERVER

Caz
AUTHORIZATION CODE GRANT TYPE FLOW IN OAUTH2 bytes

CLIENT AUTH SERVER RESOURCE SERVER
cs cx
Poss cx

USER
@ =, _

C1] | want to access my resources

Tell the Auth Server that you are fine 6

to do this action
Hello Auth Server pls allow the client to access my resources.

Here are my cred@ntials to prove my identity

Hey Client, User allowed you to access his (4)
resources. Here is AUTHORIZATION CODE

Here are my client credentials, AUTHZ
CODE. Please provide me an access token

Here is the access token from Auth Server

Hey Resource Server, | want to access the uger resources. Here is the access token from

Authz server

Hey Client, your token is validated successfully. Here are the resources you HEIRS |

Caz
AUTHORIZATION CODE GRANT TYPE FLOW IN OAUTH2 bytes

Y Inthe steps 2 & 3, where client is making a request to Auth Server endpoint have to send the below important details,

client_id — the id which identifies the client application by the Auth Server. This will be granted when the client register
first time with the Auth server.

redirect_uri — the URI value which the Auth server needs to redirect post successful authentication. If a default value is
provided during the registration then this value is optional

scope -— similar to authorities. Specifies level of access that client is requesting like READ

state — CSRF token value to protect from CSRF attacks

response_type — With the value ‘code’ which indicates that we want to follow authorization code grant

Y Inthe step 5 where client after received a authorization code from Auth server, it will again make a request to Auth server
for a token with the below values,

code -— the authorization code received from the above steps

client_id & client_secret — the client credentials which are registered with the auth server. Please note that these are
not user credentials

grant_type — With the value ‘authorization_code’ which identifies the kind of grant type is used
redirect_uri


eaz
bytes

AUTHORIZATION CODE GRANT TYPE FLOW IN OAUTH2

v¥ We may wonder that why in the Authorization Code grant type client is making request 2 times to Auth

server for authorization code and access token.

¢ In the first step, authorization server will make sure that user directly interacted with it along with
the credentials. If the details are correct, auth server send the authorization code to client

* Once it receives the authorization code, in this step client has to prove it’s identity along with the

authorization code & client credentials to get the access token.

Y Well you may ask why can’t Auth server directly club both the steps together and provide the token in a
single step. The answer is that we used to have that grant type as well which is called as

‘implicit grant type’. But this grant type is deprecated as it is less secure.


SECURING GATEWAY USING AUTHORIZATION CODE GRANT TYPE eazy
FLOW IN OAUTH2 y

AUTH SERVER (4)

Oo AK

Edge server validates the
access token issued by Auth

server

Client invokes a path in

edge server using access
token issued by Auth server request to the actual
services

@) © UI Web Accounts
Gia App/Mobile microservice
App
End user uses a website/app &
try to access a secure page Cards

where it invokes APIs hosted microservice
ee CLIENT RESOURCE SERVER

at Gateway server

Client app forwards the
request to KeyCloak where

User enters his credentials
on a login page. Post
successful authentication,

access token will be
received by client

(=)

If the access token valid, Loans
edge server allows the microservice

Unsecured services deployed
behind the docker network or

Kubernetes firewall network. So
can’t be accessed directly

** When ever an end user involved & trying to communicate with
Spring Cloud Gateway, then we need to use the OAuth2
Authorization Code grant flow for Authentication & Authorization.

SECURING GATEWAY USING AUTHORIZATION CODE GRANT TYPE
FLOW _IN OAUTH2

RESOURCE SERVER

1 LL Client app trying to invoke /eazybank/accounts/api/** with

out any authentication on behalf of end user

Gateway/Resource server replies, “Sorry Buddy, I can only

process the requests who provide a access token from Auth
server. Go and get a access token from Auth Server’.

3 Client application asked KeyCloak which is a Auth Server for Access
token

token from me, you and your partner in crime (end user) need to register with me and the same
needs to be approved by my admin

CLIENT

Auth Server laughed and replied, | can’t give access token just like that. In order to get access 4

eaz
bytes

ZiKEYCLOAK

AUTH SERVER

SECURING GATEWAY USING AUTHORIZATION CODE GRANT TYPE eazy
FLOW IN OAUTH2 y

RESOURCE SERVER

Gateway shared the received
21 access token to the Auth
Server to confirm whether it is

valid or not

Client app trying to invoke /eazybank/accounts/api/**
with the access token that it received during the step 9

Gateway/Resource server invokes the actual microservic ere tater eet flaereeeed ieeeeieties
API & respond back with the successful response 13 the Gateway that the access 12

token is valid

5 Client application asked KeyCloak which is a Auth Server for Access token. But this time, it passed Client ID which it
received during the registration process. The KeyCloak shows the login page for the end user

ZiKEYCLOAK

End user says “Hello Auth Server, pls allow the client app to access the secured resources on my behalf. Here are my
credentials to prove my identity

Hey Client, User allowed you to access resources on his
behalf. Here is AUTHORIZATION CODE

Hey Auth Server, here are my Client ID, Client Secret & AUTHORIZATION
CODE which shared in step 7. Please provide me a token

Auth Server replied, “Congratulations Buddy. Your details are correct.
Here is the end user access token. Don’t misuse it. All the Best”

CLIENT AUTH SERVER

wpa: . . . eazy
Building Event-driven microservices bytes

Using asynchronous

communication

Synchronous communication
~ ; between services is not always
Avoiding temporal coupling

: necessary. In many real-world
whenever possible

scenarios, asynchronous Building event driven
Temporal coupling occurs when a et oe iy hs, ms microservices
caller service expects an immediate requirements emectively: 20, Now _. ——
xS response from a callee service before continuing can we establish asynchronous An event, as an incident, signifies a
og its processing. If the callee experiences any delay communication between services? significant occurrence within a system, such

in responding, it negatively impacts the overall as a State transition. Multiple sources can
response time of the caller. This scenario

generate events. When an event takes place,

commonly arises in synchronous communication it is possible to alert the concerned parties.

between services. How can we prevent temporal How can one go about constructing event-
coupling and mitigate its effects? driven

services with these

characteristics?

Event-driven microservices can be built using Event-driven architecture, producing and
consuming events using Async communication, event brokers, Spring Cloud Function,
Spring Cloud Stream. Let’s explore the world of event-driven microservices

eazy
Event-driven models

bytes

Event-driven architectures can be built using two primary models

Publisher/Subscriber (Pub/Sub) Event Streaming Model
Model

In this model, events are written to a log
in a sequential manner. Producers publish
events as they occur, and these events
are stored in a well-ordered fashion.
Instead of subscribing to events,
consumers have the ability to read from
any part of the event stream. One
advantage of this model is that events
can be replayed, allowing clients to join

at any time and receive all past events.
bbRabbitMQ. 88 katka

This model revolves around
subscriptions. Producers generate events
that are distributed to all subscribers for
consumption. Once an event is received,

it cannot be replayed, which means new
subscribers joining later will not have
access to past events.

The pub/sub model is frequently paired with RabbitMQ as a popular

option. On the other hand, Apache Kafka is a robust platform widely
utilized for event stream processing.

, , , eazy
What we are going to build using a pub/sub model bytes

The event broker receives the event
details and push the same into a

queue which the message service
subscribed

Some one try to
create a new account

@)

End user receives
response that account
created successfully

accounts microservice Event Broker message service

@)

The event broker receives the event
details and push the same into a

queue which the accounts service
subscribed


eaz
Using RabbitMQ for publish/subscribe communications bytes

RabbitMQ, an open-source message broker, is widely recognized for its utilization of AMQP (Advanced Message Queuing Protocol) and its ability
to offer flexible asynchronous messaging, distributed deployment, and comprehensive monitoring. Furthermore, recent versions of RabbitMQ

have incorporated event streaming functionalities into their feature set.

When using an AMQP-based solution such as RabbitMQ, the participants engaged in the interaction can be classified into the following

categories:

Me Producer: The entity responsible for sending messages (also known as the publisher).

Ed Consumer: The entity tasked with receiving messages (also known as the subscriber).

= ag Message broker: The middleware that receives messages from producers and directs them

to the appropriate consumers.

Receive or subcribe message
from

Producer Send or Produce message to Message
Broker


eaz
Using RabbitMQ for publish/subscribe communications bytes

The messaging model of AMQP operates on the principles of excha nges and QUEUES, as depicted in the following illustration.
Producers transmit messages to an exchange. Based on a specified routing rule, RabbitMQ determines the queues that should

receive a copy of the message. Consumers, in turn, read messages from a queue.
Send or Produce
message to
Producer

/ \
ceive or subcribe message \

\
from / 4
, Consumer 1 ,
\ ,
q 7
a Wessaaae /
Exchange --------
/ \
/ \
een pPceive or subcribe messag¢

\
from

7 \
, Consumer 2 }
A

, , eazy
Why to use Spring Cloud Function ? bytes

Spring Cloud Function facilitates the development of business logic by utilizing functions that adhere to the standard interfaces introduced in
Java 8, namely Supplier, Function, and Consumer.

Supplier: A supplier is a function that produces an output without requiring any input. It can
also be referred to as a producer, publisher, or source.

Function: A function accepts input and generates an output. It is commonly referred to as a
processor.

Consumef: A consumer is a function that consumes input but does not produce any output. It
can also be called a subscriber or sink.

x & *

Spring Cloud Function features:

* Choice of programming styles - reactive, imperative or hybrid.

¢ POJO functions (i.e., if something fits the @Functionallnterface semantics we'll treat it as function)

¢ Function composition which includes composing imperative functions with reactive.

¢ REST support to expose functions as HTTP endpoints etc.

* Streaming data (via Apache Kafka, Solace, RabbitMQ and more) to/from functions via Spring Cloud Stream framework.

¢ Packaging functions for deployments, specific to the target platform (e.g., AWS Lambda and possibly other "serverless" service
providers)

eazy

Steps to create functions using Spring Cloud Functions bytes

Below are the steps to create functions using Spring Cloud Functions,

@)
@)

Start by creating a new Spring Boot project using your preferred IDE or by using Spring Initializr

(https://start.spring.io/). Include the

Develop two functions with the name email() and
sms() like in the image. To make it simple, for now
they just have logic of logging the details. But in
real projects you can write logic to send emails and
messages.

To enable Spring Cloud Function to recognize our
functions, we need to register them as beans.
Proceed with annotating the MessageFunctions
class as @Configuration and the methods email() &
sms() as @Bean to accomplish this.

maven dependency

@Configuration

public class MessageFunctions {
private static final Logger log = LoggerFactory.getLogger(MessageFunctions.class);

@Bean
public Function<AccountsMsgDto, AccountsMsgDto> email() {
return accountsMsgDto -> {
log.info( + accountsMsgDto.toString());

return accountsMsgDto;

@Bean

public Function<AccountsMsgDto, Long> sms() q

return accountsMsgDto -> {
log.info( + accountsMsgDto.toString());

return accountsMsgDto.accountNumber();

eazy
Steps to create functions using Spring Cloud Functions bytes

Composing functions: If our scenario needs multiple functions to be executed, then we need to compose them otherwise we can use them
as individual functions as well. Composing functions can be achieved by defining a property in application.yml like shown below,

spring:
cloud:
function:
definition: email|sms

The property spring.cloud.function.definition enables you to specify which functions should be managed and integrated by Spring Cloud Function,
thereby establishing a specific data flow. In the previous step, we implemented the email() and sms() functions. We can now instruct Spring Cloud
Function to utilize these functions as building blocks and generate a new function derived from their composition.

In serverless applications designed for deployment on FaaS platforms like AWS Lambda, Azure Functions, Google Cloud Functions, or Knative, it is
common to have one function defined per application. The definition of cloud functions can align directly with functions declared in your
application on a one-to-one basis. Alternatively, you can employ the pipe (|) operator to compose functions together in a data flow. In cases where
you need to define multiple functions, the semicolon (;) character can be used as a separator instead of the pipe (|).

Based on the provided functions, the framework offers various ways to expose them according to our needs. For instance, Spring Cloud
Function can automatically expose the functions specified in spring.cloud.function.definition as REST endpoints. This allows you to package
the application, deploy it on a FaaS platform such as Knative, and instantly have a serverless Spring Boot application. But that is not what we

want. Moving forward, the next step involves integrating it with Spring Cloud Stream and binding the function to message channels
within an event broker like RabbitMQ.


eaz
Why to use Spring Cloud Stream ? bytes

Spring Cloud Stream is a framework designed for creating scalable, event-driven, and streaming applications. Its core principle is to allow
developers to focus on the business logic while the framework takes care of infrastructure-related tasks, such as integrating with a message
broker.

Spring Cloud Stream leverages the native capabilities of each message broker, while also providing an abstraction layer

to ensure a consistent experience regardless of the underlying middleware. By just adding a dependency to your project,
you can have functions automatically connected to an external message broker. The beauty of this approach is that you
don't need to modify any application code; you simply adjust the configuration in the application.yml file.

The framework supports integrations with RabbitMQ, Apache Kafka, Kafka Streams, and Amazon Kinesis.
There are also integrations maintained by partners for Google PubSub, Solace PubSub+, Azure Event Hubs,
and Apache RocketMQ.

The core building blocks of Spring Cloud Stream are:

Destination Binders: Components responsible to provide integration with the external
messaging systems.

(producer/consumer) provided by the end user.

Message: The canonical data structure used by producers and consumers to communicate with Destination Binders (and thus

other applications via external messaging systems).

EG Destination Bindings: Bridge between the external messaging systems and application code

eaz
Why to use Spring Cloud Stream ? bytes

Spring Cloud Stream equips a Spring Boot application with a destination binder that seamlessly integrates with an external messaging system.
This binder takes on the responsibility of establishing communication channels between the application's producers and consumers and the
entities within the messaging system (such as exchanges and queues in the case of RabbitMQ). These communication channels, known as
destination bindings, serve as connections between applications and brokers.

A destination binding can function as either an input channel or an output channel. By default, Spring Cloud Stream maps each binding, both
input and output, to an exchange within RabbitMQ (specifically, a topic exchange). Additionally, for each input binding, it binds a queue to the
associated exchange. This queue serves as the source from which consumers receive and process events. This configuration provides the
necessary infrastructure for implementing event-driven architectures based on the pub/sub model.

Spring Boot Application

Destination Binders

Input Destination Output Destination h
Exchange Binding Binding Exchange


eazy

Steps to create bindings using Spring Cloud Stream bytes

Below are the steps to create bindings using Spring Cloud Stream,

@)
@)

We need to define input binding for each function accepting input data, and an output binding for
each function returning output data. Each binding can have a logical name following the below
convention. Unless you use partitions (for example, with Kafka), the <index> part of the name will
always be 0. The <functionName> is computed from the value of the spring.cloud.function.definition

property.

inside pom.xml of message service where we defined functions

Add the stream binding and rabbitmgq properties inside application.yml of
message service

Input binding: <functionName> + -in- + <index>
Output binding: <functionName> + -out- + <index>

The binding names exist only in Spring Cloud Stream and RabbitMQ doesn’t know about them.
So to map between the Spring Cloud Stream binding and RabbitMQ, we need to define
destination which will be the exchange inside the RabbitMQ. group is typically application
name, so that all the instances of the application can point to same exchange and queue.

The queues will be created inside RabbitMQ based on the queue-naming strategy
(<destination>.<group>) includes a parameter called consumer group.

Add the Stream related dependencies: Add the maven dependencies spring-cloud-stream, spring-cloud-stream-binder-rabbit

spring:
application:
name: message
cloud:
function:
definition: email|sms
stream:
bindings:
emailsms-in-0O:
destination: send-communication
group: ${spring.application.name}
emailsms-out-0:
destination: communication-sent
rabbitmd:
host: localhost
port: 5672
username: guest
Password: guest
connection-timeout: 10s

, . , , eazy
Event producing and consuming in accounts microservice bytes

Below are the steps for event producing and consuming in accounts microservice

Autowire StreamBridge class: StreamBridge is a class inside Spring Cloud Stream which allows user to send data to an output binding. So to
produce the event, autowire the StreamBridge class into the class from where you want to produce a event

Use send() of StreamBridge to produce a event like shown below,

c void createAccount(CustomerDto customerDto) {
Customer customer = CustomerMapper.map ToCustomer(customerDto, new Customer()):
Optional<Customer> optionalCustomer = customerRepository.findByMobileNumber(

customerDto.getMobileNumber());

f (optionalCustomer.isPresent()) {

throw new CustomerAlreadyExistsException(

+ customerDto.getMobileNumber());

}
Customer savedCustomer = customerRepository.save(customer);
Accounts savedAccount = accountsRepository.save(createNewAccount(savedCustomer));

sendCommunication(savedAccount, savedCustomer);

private void sendCommunication(Accounts account, Customer customer) {
var accountsMsgDto = new AccountsMsgDto(account.getAccountNumber(), customer.getName(),
customer.getEmail(), customer.getMobileNumber());
log.info( , accountsMsgDto);

var result = streamBridge.send( , accountsMsgDto);

log.info( , result);


eazy

Event producing and consuming in accounts microservice bytes
re Functi 7 the event: Inside accounts microservice, we need to create a function that accepts the event and update the
communication status inside the DB. Below! is a sample code snippet of the same

@Configuration

public class AccountsFunctions {

private static final Logger /og = LoggerFactory.getLogger(AccountsFunctions.class);

@Bean
public Consumer<Long> updateCommunication(I[AccountsService accountsService) {
return accountNumber -> {
log.info("Updating Communication status for the account number : " + accountNumber.toString()):
accountsService.updateCommunicationStatus(accountNumber);

y


, . , , eazy
Event producing and consuming in accounts microservice bytes

(4) Add the stream binding and rabbitmg properties inside application.yml of accounts service

when accounts microservice want to produce a event using StreamBridge, we should have spring:
a supporting stream binding and destination. The same we created with the names application:
sendCommunication-out-0 and send-communication Mame: "accounts"
cloud:
Similarly we need to define input binding for the function upbdateCommunication to accept function:
the event using the destination communication-sent. So when the message service push a definition: updateCommunication

event into the exchange of communication-sent, the same will be processed by the function
updateCommunication Sas
bindings:
updateCommunication-in-0:
destination: communication-sent
group: ${spring.application.name}
sendCommunication-out-0:
destination: send-communication
rabbitmd:
host: localhost
port: 5672
username: guest
Password: guest
connection-timeout: 10s


eazy
Apache Kafka Vs RabbitMQ bytes

Kafka and RabbitMQ are both popular messaging systems, but they have some fundamental differences in terms of design philosophy,
architecture, and use cases. Here are the key distinctions between Kafka and RabbitMQ:

Design: Kafka is a distributed event streaming platform, while RabbitMQ is a message broker. This means that Kafka is designed to handle
large volumes of data, while RabbitMQ is designed to handle smaller volumes of data with more complex routing requirements.

Data retention: Kafka stores data on disk, while RabbitMQ stores data in memory. This means that Kafka can retain data for longer
periods of time, while RabbitMQ is more suitable for applications that require low latency.

Performance: Kafka is generally faster than RabbitMQ, especially for large volumes of data. However, RabbitMQ can be more performant
for applications with complex routing requirements.

Scalability: Kafka is highly scalable, while RabbitMQ is more limited in its scalability. This is because Kafka can be scaled horizontally to
any extent by adding more brokers to the cluster

x * * *

Ultimately, the best choice for you will depend on your specific needs and requirements. If you need a high-performance messaging
system that can handle large volumes of data, Kafka is a good choice. If you need a messaging system with complex routing requirements,
RabbitMQ is a good choice.

Introduction to Apache Kafka

Source of Data (Producers)

Destination of Data (Consumers)

aa

7 a

eaz
bytes

eazy
Introduction to Apache Kafka bytes

Apache Kafka is an open-source distributed event streaming platform. It is designed to handle large-scale, real-time data streams and enables
high-throughput, fault-tolerant, and scalable data processing. It is used to build real-time streaming data pipelines and applications that adapt to
the data streams.

Here are some key concepts and components of Kafka:

Producers: Producers are responsible for publishing messages to Kafka topics. They write
messages to a specific topic, and Kafka appends these messages to the topic's log.

Topics: Kafka organizes data into topics. A topic is a particular stream of data that can be divided
into partitions. Each message within a topic is identified by its offset.

Brokers: Brokers are the Kafka servers that manage the storage and replication of topics. They
are responsible for receiving messages from producers, assigning offsets to messages, and serving
messages to consumers.

Partitions: Topics can be divided into multiple partitions, allowing for parallel processing and load
balancing. Each partition is an ordered, immutable sequence of messages, and each message within a
partition has a unique offset.

Offsets: offsets are unique identifiers assigned to each message within a partition. They are used to track the progress of
consumers. Consumers can control their offsets, enabling them to rewind or skip messages based on their needs.

x * & K *


eazy
Introduction to Apache Kafka bytes

Replication: Kafka allows topics to be replicated across multiple brokers to ensure fault tolerance. Replication provides data
redundancy, allowing for failover and high availability.

Consumers: Consumers read messages from Kafka topics. They subscribe to one or more topics and consume messages by reading
from specific partitions within those topics. Each consumer maintains its offset to track its progress in the topic.

Consumer Groups: Consumers can be organized into consumer groups. Each message published to a topic is delivered to only one
consumer within each group. This enables parallel processing of messages across multiple consumers.

Streams: Kafka Streams is a client library that enables stream processing within Kafka. It allows you to build applications that consume,

transform, and produce data in real-time.

x * * *

cae

eazy
Introduction to Apache Kafka bytes

Kafka Cluster with group of brokers

Broker 1

Consumer Group A

Consumer 1

MX) Partitions Pull messages

en DEAD pol? }*{273/*}s]ei7] 2]? as —
ee ott tere rey syed 7 tele
Producer 2 P1

Messages get stored in partitions using offset ids

MX) 4)

Consumer 2

Producer 3

bh Consumer Group B

MX] MX) Consumer 4

Broker 2 Pull messages

Consumer 5

Consumer 6

Broker 3


Introduction to Apache Kafka

*
*

*

eaz
bytes

A Kafka cluster can have any number of producers, consumers, brokers. For a production set up, atleast 3 brokers is recommended.
This helps in maintaining replications, fault tolerant system etc.

A Kafka broker can have any number of topics. Topic is a category under which producers can write and interested, authorized consumers can
read data. For example, we can have topics like sendCommunication, dispatchOrder, purgeData etc.

Kafka Cluster with group of brokers

Inside each topic, we can have any number of partitions. Why do we need partitions ? erokert

Since Kafka producers can handle enormous amount of data, it is not Topics
possible to store in a single server (broker). Therefore, a topic will be sl WEI Soa TEP
partitioned into multiple parts and distributed across multiple a

brokers, since Kafka is a distributed system. For example, we can oe
store all customers data from a state, zipcode, region etc. inside a
partition and the same can be replicated as per the configurations.

Consumer Group A

Pull messages

Push messages

PPT sl it]:
Messages get stored in partitions using offset ids

Broker 2

Broker 3

Offsets is a sequence id assigned to a message as they get stored inside a partition.
The offset number starts from 0 and followed by 1,2,3.... Once offset id is assigned, it will
never change. These are similar to sequence ids inside the DB tables.

By keeping track of offsets, Kafka provides reliability, fault tolerance, and flexibility to consumers. Consumers have fine-grained control
over their progress, enabling them to manage message ordering, replay messages, ensure message delivery, and facilitate parallel
processing.

Producer side story

©

©
©
©

Producer Configuration: Before pushing a message into Kafka,

a producer needs to be configured. This involves setting up
properties such as the Kafka broker addresses, serialization format
for messages, and other optional configurations like compression
or batching.

Topic Selection: The producer needs to specify the topic to which
it wants to push the message. Topics are predefined streams of
data within Kafka. If the topic doesn't exist, it can be created
dynamically, depending on the broker's configuration.

Message Production: The producer sends the message to Kafka by
using the Kafka client library's API. The producer specifies the
target topic and the serialized message. It may also provide a
partition key (optional) to control which partition the message
should be written to.

Partition Assignment: If a partition key is provided, Kafka uses it

to determine the target partition for the message. If no partition

key is provided, Kafka uses a round-robin or hashing algorithm to
distribute messages evenly across partitions.

©

©
©

eaz
bytes

Message Routing & offset assignment: The producer sends the
message to the appropriate Kafka broker based on the target topic
and the partition assigned to the message. The broker receives the
message and appends it to the log of the corresponding partition
in a durable and ordered manner with the help of offset id.

Message Replication: Kafka ensures high availability and fault
tolerance by replicating messages across multiple brokers. Once
the message is written to the leader partition, Kafka
asynchronously replicates it to other replicas of the partition.

Acknowledgment and Error Handling: The producer receives an
acknowledgment from Kafka once the message is successfully
written to the leader partition. The producer can handle any
potential errors, retries, or failures based on the acknowledgment
received. Depending on the acknowledgment mode configured,
the producer may wait for acknowledgment from all replicas or
just the leader replica.

Consumer side story

QO O OVO O

Consumer Group and Topic Subscription: Consumers in Kafka are
typically organized into consumer groups. Before reading
messages, a consumer needs to join a consumer group and
subscribe to one or more topics. This subscription specifies which
topics the consumer wants to consume messages from.

Partition Assignment: Kafka assigns the partitions of the
subscribed topics to the consumers within the consumer group.
Each partition is consumed by only one consumer in the group.
Kafka ensures a balanced distribution of partitions among
consumers to achieve parallel processing.

Offset Management: Each consumer maintains its offset for each
partition it consumes. Initially, the offset is set to the last
committed offset or a specified starting offset. As the consumer

reads messages, it updates its offset to keep track of the progress.

Fetch Request: The consumer sends fetch requests to the Kafka
broker(s) it is connected to. The fetch request includes the topic,
partition, and the offset from which the consumer wants to read
messages. The request also specifies the maximum number of
messages to be fetched in each request.

QO OO ©

eaz
bytes

Message Fetching: Upon receiving the fetch request, the Kafka
broker retrieves the requested messages from the corresponding
partition's log. It sends the messages back to the consumer ina
fetch response. The response contains the messages, their
associated offsets, and metadata.

Message Processing: Once the consumer receives the messages, it
processes them according to its application logic. This processing
can involve transformations, aggregations, calculations, or any
other operations based on the business requirements.

Committing the Offset: After successfully processing a batch of
messages, the consumer needs to commit the offset to Kafka. This
action signifies that the consumer has completed processing the
messages up to that offset. Committing the offset ensures that the
consumer's progress is persisted and can be resumed from that
point in case of failure or restart.

Polling Loop: The consumer repeats the process of sending fetch
requests, receiving messages, processing them, and committing
the offset in a continuous loop. This loop allows the consumer to
continuously consume and process new messages as they become
available.

eazy

Steps to use Apache Kafka in the place of RabbituQ bytes

Below are the steps to use Apache Kafka in the place of RabbitMQ,

G) Add maven dependencies: Add the maven dependency spring-cloud-stream-binder-kafka in the place of spring-cloud-stream-binder-

rabbitmg dependency

(2) Add Kafka related properties inside the application.yml file of both accounts and message services

spring:
application:
name: "accounts"
cloud:
function:
definition: updateCommunication
stream:
bindings:
updateCommunication-in-0:
destination: communication-sent
group: ${spring.application.name}
sendCommunication-out-0:
destination: send-communication
kafka:
binder:
brokers:
- localhost:9092

spring:
application:
name: message
cloud:
function:
definition: email |sms
stream:
bindings:
emailsms-in-0:
destination: send-communication
group: S{spring.application.name}
emailsms-out-0:
destination: communication-sent
kafka:
binder:
brokers:
- localhost:9092

eaz
CONTAINER ORCHESTRATION bytes

MAKING SURE OUR

SERVICES ARE SELF-HEALING?

How do we automatically restarts
AUTOMATING THE containers that fail, replaces

containers, kills containers that
DEPLOYMENTS, ROLLOUTS &

don't respond to your user-defined
ROLLBACKS? health check, and doesn't advertise AUTOSCALING

them to clients until they are ready OUR SERVICES ?

to serve.

xS How do we automate deployment of
oJ the containers into a complex cluster

How do we monitor our services
env and perform rollout of new versions and scale them automatically
of the containers with out down time based on metrics ike CPU
along with an option of automatic Utilization etc. ?
rollback in case of any issues?

Kubernetes is an open-source container orchestration platform that automates the
deployment, scaling, and management of containerized applications. It was originally
developed by Google and is now maintained by the Cloud Native Computing Foundation
(CNCF).


eaZz
WHAT IS KUBERNETES ? bytes

Kubernetes, is an open-source system for automating deployment, scaling, and managing containerized applications. It is the most famous
orchestration platform and it is cloud neutral.

Google open-sourced the Kubernetes project in 2014. Kubernetes combines over 15 years of Google’s
experience running production workloads at scale with best-of-breed ideas and practices
from the community.

Kubernetes provides you with a framework to run distributed systems resiliently.
It takes care of scaling and failover for your application, provides deployment
patterns, and more. It provides you with:

¢ Service discovery and load balancing

¢ Container & storage orchestration

¢ Automated rollouts and rollbacks

¢ Self-healing

* Secret and configuration management

The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an
abbreviation results from counting the eight letters between the "K" and the "s".

eaz
KUBERNETES INTERNAL ARCHITECTURE bytes

ne

Admin UI _ kubectl CLI

Control Panel / Master Node

Control
Manager

252
ON

ry

Container runtime (Docker) Container runtime (Docker)
User or Clients can

access services inside pod1 pod2 pod1

K8s using kube-proxy

Worker Node 1 Worker Node 2


Components of Control Panel (Master Node)

The master node is responsible for managing an entire cluster. It monitors the health check of all the nodes in the cluster, stores members’
information regarding different nodes, plans the containers that are scheduled to certain worker nodes, monitors containers and nodes, etc. So,

when a worker node fails, the master moves the workload from the failed node to another healthy worker node.

Below are the details of the four basic components present inside the control panel,

*
*
*
*

API server - The API server is the primary interface for interacting with the Kubernetes cluster. It exposes
the Kubernetes API, which allows users and other components to communicate with the cluster. All
administrative operations and control commands are sent to the API server, which then processes and

validates them.

Scheduler - The scheduler is responsible for placing Pods onto available nodes in the cluster. It takes into
account factors like resource requirements, affinity, anti-affinity, and other constraints to make intelligent
decisions about which node to assign a Pod to. The scheduler continuously monitors the cluster and ensures

that Pods are distributed optimally.

Controller manager - The controller manager maintains the cluster. It handles node failures, replicates
components, maintains the correct number of pods, etc. It constantly tries to keep the system in the desired

state by comparing it with the current state of the system.

etcd - etcd is a distributed key-value store that serves as the cluster's primary data store. It stores the
configuration data and the desired state of the system, including information about Pods, Services,
ReplicationControllers, and more. The API server interacts with etcd to read and write cluster data.

eaz
bytes


eazy
Components of Worker Node bytes

The worker node is nothing but a virtual machine (VM) running in the cloud or on-prem (a physical server running inside your data center). So,
any hardware capable of running container runtime can become a worker node. These nodes expose underlying compute, storage, and
networking to the applications. Worker nodes do the heavy-lifting for the application running inside the Kubernetes cluster. Together, these
nodes form a cluster — a workload assign is run to them by the master node component, similar to how a manager would assign a task to a team
member. This way, we will be able to achieve fault-tolerance and replication.

Pods are the smallest unit of deployment in Kubernetes just as a container is the smallest unit of deployment in Docker. To
understand in an easy way, we can say that pods are nothing but lightweight VMs in the virtual world. Each pod consists of
one or more containers. Each time a pod spins up, it gets a new IP address with a virtual IP range assigned by the pod
networking solution.

Below are the details of the three basic components present inside the worker node,

Kubelet is an agent that runs on each worker node and communicates with the control plane components. It
receives instructions from the control plane, such as Pod creation and deletion requests, and ensures that the
desired state of Pods is maintained on the node. The kubelet is responsible for starting, stopping, and monitoring
containers based on Pod specifications.

Kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the
Kubernetes Service concept. kube-proxy maintains network rules on nodes. These network rules allow
network communication to your Pods from network sessions inside or outside of your cluster.

Container Runtime is responsible for running and managing containers on a worker node. Kubernetes supports multiple container
runtimes, with Docker being the most commonly used. Other runtimes like containerd and rkt are also supported. The container runtime
pulls container images, creates and manages container instances, and handles container lifecycle operations.

eazy

Kubernetes manifest file to create ConfigMap bytes

A Kubernetes ConfigMap is an essential Kubernetes resource used to store configuration data separately

from the application code.

v1
ConfigMap

eazybank-configmap

prod

The apiVersion and kind fields are required for all
Kubernetes objects. When creating a config map
inside K8s, the kind should be “ConfigMap”

The metadata field contains the name of the
ConfigMap and other metadata about the object.

The data field is where the key-value pairs are stored.
The keys can be any alphanumeric string, and the
values can be strings, numbers, or binary data.

€aZz
Kubernetes manifest file to deploy a Container bytes

In Kubernetes, a Deployment is a high-level resource used to manage the deployment and scaling of containerized applications. It
provides a declarative way to define and maintain the desired state of your application. When you create a Deployment, Kubernetes
ensures that the specified number of replicas of your application are running and automatically handles scaling, rolling updates, and
rollbacks.

esloyent The apiVersion and kind fields are required for all Kubernetes objects. For

deployment manifest file, the kind should be “Deployment”

accounts-—depLoyment

accounts Metadata: The metadata section contains information about the Deployment, such
as its name and labels. The Deployment is named "accounts-deployment", and it
has the label "app: accounts".

accounts

Spec: The spec section defines the desired state of the Deployment.

accounts

Replicas: The replicas field is set to 1, indicating that only one replica of the
container should be running at any given time.

accounts

eazybytes/accounts: latest

Selector: The selector field is used to select the pods controlled by this

Deployment. In this case, it's using the label "app: accounts" to identify the pods.
SPRING_PROFILES_ACTIVE

eazybank-configmap
SPRING_PROFILES_ACTIVE


eazy

Kubernetes manifest file to deploy a Container bytes

apiVersion: apps/v1
kind: Deployment
metadata
name: accounts-—depLoyment
labels
app: accounts
spec
replicas: 1
selector
matchLabels
app: accounts
template
metadata

labels
app: accounts
spec
containers

name: accounts
image: eazybytes/accounts: latest
ports
containerPort: 8080
env
name: SPRING_PROFILES_ACTIVE
vaLueFrom
configMapKeyRef
name: eazybank-configmap
key: SPRING_PROFILES_ACTIVE

The template section specifies the pod template that will be used to
create pods for this Deployment.

The metadata section inside the template defines the labels for the
pods. The pod will have the label “app: accounts".

The spec section inside the template specifies the details of the pod's
specification.

The containers field lists the containers that should be part of the pod.
In this case, there is one container named "accounts" based on the
“eazybytes/accounts:latest" image.

The ports section exposes port 8080 of the container.

The env section sets an environment variable named
"SPRING PROFILES ACTIVE" and assigns it a value from a ConfigMap.

The valueFrom field allows you to reference data from a ConfigMap. In
this case, it is using configMapKeyRef to fetch the value of the
"SPRING PROFILES ACTIVE" key from the ConfigMap named "eazybank-
configmap".

eazy

Kubernetes manifest file to create a service bytes

In Kubernetes, a Service is an essential resource that provides network connectivity to a set of pods. It acts as a stable endpoint for
accessing and load balancing traffic across multiple replicas of a pod. Services abstract away the underlying network details, allowing
pods to be more dynamic and scalable without affecting how clients access them.

apiVersion: vl
kind: Service
metadata
name: accounts-service
spec

selector
app: accounts
type: ClusterIP
ports
protocol: TCP
port: 8080
targetPort: 8080

The and fields are required for all Kubernetes objects. For service
manifest file, the kind should be “Service”

The metadata section contains information about the Service, such as its
name.

The spec section defines the desired state of the Service.
The selector field is used to select the pods that the Service will route
traffic to. In this case, it uses the label "app: accounts" to select the pods controlled

by the Deployment with the same label.

The type field specifies the type of Service. In this case, it is set to "ClusterIP,"
which means that the Service will be accessible only from within the cluster.

eazy

Kubernetes manifest file to create a service bytes

vl
Service

accounts-service

accounts
ClusterIP

TCP

Ports: The ports section defines the ports that the Service should listen on and
forward traffic to.

protocol: The protocol field specifies the protocol used for the service port. In this
case, it's TCP.

port: The port field is the port number on which the Service will listen for incoming
traffic.

targetPort: The targetPort field is the port number on the pods to which the
incoming traffic will be forwarded.

How K8s deployment & services tied together

10.23.12.45 (Cluster IP)

accounts-

service

@)

I
I
I
I
I
I
I
I
I
|

I

I

I

@-e

I

| 10.1.0.1 10.1.0.2

I

ee i
WORKER NODE 1

KUBERNETES CLUSTER

ee ee ee

@)

apiVersion: apps/vl

kind: Deployment

metadata PX he
name: accounts-depLoyment er
labels we

7 ws

.
»**
*
.

app: accounts
spec
replicas: 1
selector
matchLabels
app: accounts
template
metadata
labels
app: accounts
spec
containers
name: accounts
image: eazybytes/accounts: Latest
ports
containerPort: 8080

eaz
bytes

apiVersion: vl
kind: Service
metadata
name: accounts-service
spec
selector

4 app: accounts
type: ClusterIP
ports
protocol: TCP
port: 8080
targetPort: 8080

K8s Deployment manifest will give instructions to
deploy the containers into a pod inside one of
the worker node of the K8s cluster

K8s Service manifest will give instructions to
create a service inside the K8s cluster which
tracks service registration & discovery of a
specific service/deployment

The binding between a container running inside
a Pod and a service will be done based “app”
label & selector inside the manifest files.

Kubernetes Service Types

ClusterIP Service

eterna aint aincernie
This is the default service that uses an
internal Cluster IP to expose Pods. In
ClusterIP, the services are not available for
external access of the cluster and used for
internal communications between
different Pods or microservices in the

cluster.

NodePort Service
tae

This service exposes outside and allows
the outside traffic to connect to K8s Pods

through the node port which is the port

opened at Node end. The Pods can be

accessed from external using

<Nodelp>:<Nodeport>

eaz
bytes

LoadBalancer Service
LT CINTA TT NE a ee eras

This service is exposed like in NodePort
but creates a load balancer in the cloud
where K8s is running that receives external
requests to the service. It then distributes
them among the cluster nodes using

NodePort.

K8s CLUSTER IP SERVICE

eaz
bytes

ClusterIP service creates an internal IP address for use within the K8s cluster.
Good for internal-only applications that support other workloads within the cluster.

r a=» a=» eau» a=» a=» aap a=» a=» a=» aap a=» a=» a=» a=» a=» a=» 7

ir Eo oo 1
Pod
External Traffic not Port 8080 ]
allowed
accounts |

Worker Node 1

Port 80
Internal traffic Cluster IP

l
l
l
l
| Taina neuen at
l
l
l

ClusterIP provides a load-
balanced IP address

Tet

1

Port 8080 ] I
accounts

(—)

|

Worker Node 2

inside the K8s cluster

| aap Ld Ld bd aap aap bd Ld td aap aap bd bd aap Ld aap |

KUBERNETES CLUSTER

apps/v1
Deployment

accounts-depLoyment

accounts

accounts

accounts

accounts
eazybytes/accounts: Latest

v1
Service

accounts-service

accounts

ClusterIP

TCP

K8s NODEPORT SERVICE

eaz
bytes

Services of type NodePort build on top of ClusterlP type services by exposing the ClusterIP service outside of the cluster on high ports
(default 30000-32767). If no port number is specified then Kubernetes automatically selects a free port. The local kube-proxy is
responsible for listening to the port on the node and forwarding client traffic on the NodePort to the ClusterlP.

pe WH HX L_

NodePhrt (32593)

Cluster IP (80)

Pod

Worker Node 1

=e

External client trying to connect with
accounts service

NodePort (32593)

@

Cluster IP (80)

] Pod

Worker Node 2

a ee ee |

iY CI CY Cc > tt CY CD CD WH WH |

Leeeewewreewewew wee @® @ = @ da

KUBERNETES CLUSTER

apiVersion: vl
kind: Service
metadata
name: accounts-service
spec
selector

app: accounts
type: NodePort
ports
protocol: TCP
port: 80
targetPort: 8080
nodePort: 32593

apiVersion: apps/v1
kind: Deployment
metadata
name: accounts-depLoyment
labels
app: accounts
spec
replicas: 2
selector
matchLabels
app: accounts
template
metadata
labels
app: accounts
spec
containers
name: accounts
image: eazybytes/accounts: Latest
ports
containerPort: 8080

K8s LOADBALANCER SERVICE

eaz
bytes

The LoadBalancer service type is built on top of NodePort service types by provisioning and configuring external load balancers from
public and private cloud providers. It exposes services that are running in the cluster by forwarding layer 4 traffic to worker nodes. This
is a dynamic way of implementing a case that involves external load balancers and NodePort type services.

Ce

External client trying to connect
with accounts service

Cloud provided Load
Balancer

poe & LC LL Lh! ~~ — pd DD

| NodePort NodePort |
1 @ | 1 @ |
J cluster IP (80) ] I |
' I I I
| accounts |
8080 8080 |
i Worker Node 1 Worker Node 2 |

Leewreewerwemeemwemrwremeemeee eee @ a

KUBERNETES CLUSTER

apiVersion: vl
kind: Service
metadata
name: accounts-service
spec
selector
app: accounts
type: LoadBalancer
ports
protocol: TCP
port: 80
targetPort: 8080

apiVersion: apps/vl
kind: Deployment
metadata
name: accounts-—depLoyment
labels
app: accounts
spec
replicas: 2
selector
matchLabels
app: accounts
template
metadata
labels
app: accounts
spec
containers
name: accounts
image: eazybytes/accounts: Latest
ports
containerPort: 8080

; eazy
Introduction to Helm bytes

Male What is HELM ?

- E L M Helm is renowned as the "package manager of Kubernetes," aiming to enhance the management of Kubernetes projects
Pr by offering users a more efficient approach to handling the multitude of YAML files involved.

The path Helm tooks to solve

With out Helm we need to this issues is by using a

maintain all K8s manifest files packaging format called
for Deployment, Service, charts. A chart is a collection
ConfigMap etc. for each of files that describe a related
microservice With out Helm, the Devops set of Kubernetes resources.

team members has to
manually apply or delete all
the Kubernetes YAML

manifest files using kubectl

A single Helm chart might be used to deploy a simple app or something complex like a
full web app stack with HTTP servers, databases, caches, and so on.

A chart can have child charts and dependent charts as well. This means that Helm
can install whole dependency tree of a project with just a single command.

ala
7a, Problems that Helm solves

With out Helm, we need to maintain the separate YAML files/manifest of K8s for all microservices inside a project like below. But

majority content inside them looks similar except few dynamic values.

apiVersion: v1 apiVersion: v1
kind: Service kind: Service
metadata: metadata:

name: accounts-service name: loans-service
spec: spec:

selector: selector:

app: accounts app: loans
type: LoadBalancer type: LoadBalancer
ports: ports:
- protocol: TCP - protocol: TCP
port: 8080 port: 8090
targetPort: 8080 targetPort: 8090

accounts service manifest loans service manifest

apiVersion: v1
kind: Service
metadata:
name: cards-service
spec:
selector:
app: cards

type: LoadBalancer

ports:
- protocol: TCP
port: 9000
targetPort: 9000

cards service manifest

eaz
bytes

ala
7a, Problems that Helm solves

With Helm, we can a single template yaml file like shown below. Only the dynamic values will be injected during K8s services setup
based on the values mentioned inside the values.yaml present inside each service/chart.

apiVersion: v1
kind: Service
metadata:

deploymentLabel: accounts

name: {{ .Values.deploymentLabel }}
spec: service:
selector:

type: ClusterIP
port: 8080
targetPort: 8080

app: {{ .Values.deploymentLabel }}
type: {{ .Values.service.type }}
ports:

- protocol: TCP
port: {{ .Values.service.port }}

values.yaml
targetPort: {{ .Values.service.targetPort

Helm service template

eaz
bytes

rales
HELM Problems that Helm solves bytes

ran a)
HELM SUPPORTS PACKAGING OF YAML FILES
With the help of Helm, we can package all of the YAML manifest files (/)

belongs in to an application into a Chart. The same can be distributed — ap
into public or private repositories. S—

HELM SUPPORTS EASIER INSTALLATION —

With the help of Helm, we can set up/upgrade/rollback/remove entire

microservices applications into K8s cluster with just 1 command. No

need to manually run kubectl apply command for each manifest file. Helm acts as a package manager for K8s. Just like a
package manager is a collection of software tools that
automates the process of installing, upgrading, version

management, and removing computer
programs for a computer in a consistent manner, similarly
HELM SUPPORTS RELEASE/VERSION MANAGEMENT Helm automates the installation, rollback, upgrade of the

Helm automatically maintains the version history of the installed manifests. multiple K8s manifests with a single command.

Due to that rollback of entire K8s cluster to the previous working state is just

a single command away. TT


ew e€aZ
HELM Helm chart structure bytes

wordpress

Chart.yaml
wordpress/
—-—Chart.yml

values.yaml

—-values.yml
——charts/
—-templates/

folder contains the manifest template yaml files


rains
HELM Important Helm commands bytes

Description

helm create eazybank Create a blank chart with the name eazybank. Inside the eazybank folder, we

can see Charts.yaml, values.yml, Charts folder, templates folder etc.

helm dependencies build Build is used to reconstruct a chart's dependencies

helm install [NAME] [CHART]

Install the manifests mentioned in the [CHART] with a given release name
inside [NAME]

helm upgrade [NAME] [CHART] Upgrades a specified release to a new version of a chart
helm history [NAME] Prints historical revisions for a given release.

helm rollback [NAME] [REVISION]

Roll back a release to a previous revision. The first argument of the rollback command is the name of a release,
and the second is a revision (version) number. If this argument is omitted, it will roll back to the previous release.

helm uninstall [NAME] Removes all of the resources associated with the last release of the chart as well as the release history
helm template [NAME] [CHART] Render chart templates locally along with the values and display the output.
pheimis This command lists all of the releases for a specified namespace


€aZ
Client-side service discovery and load balancing bytes

In client-side service discovery, applications are responsible for registering themselves with a service registry like Eureka during startup and
unregistering when shutting down. When an application needs to communicate with a backing service, it queries the service registry for the
associated IP address. If multiple instances of the service are available, the registry returns a list of IP addresses. The client application then
selects one based on its own defined load-balancing strategy. Below figure illustrates the workflow of this process

oans service registers with Service registry during startup
& send regular heart beats

Eureka

Server

Dude, what are address Buddy, heere are the IP address of Loans
details of loans service ? instances,

127.54.37.23
127.54.37.24

Loans
microservice

Accounts

microservice 127.54.37.2

Accounts microservice is going to invoke one
of the instance of loans based on the load
balancing strategy configured Loans

microservice

127.54.37.2
4

€aZ
Server-side service discovery and load balancing bytes

In server-side service discovery, the K8s discovery server is responsible to monitor the application instances and maintaining the details of them.
When a microservice needs to communicate with a backing service, it simply invokes the service URL exposed by the K8s. The K8s will take care
of loadbalancing the requests at the server layer. So clients are free from load balancing in this scenario.

Below figure illustrates the workflow of this process,
Unlike Eureka, this new discovery server implementation does
GQ) not require applications to explicitly register with the server.

Instead, the discovery server uses the Kubernetes API to fetch
Kubernetes services and endpoints

K8s Discovery
Server

Loans
microservice

Accounts microservice is going to
invoke K8s service URL of loans
microservice with out worrying about

The Kubernetes service will work
(3) with Discovery server and forward
loadbalancing

the request to one of the loans
instance

Accounts

microservice K8s Service

127.54.37.2

Loans
microservice

127.54.37.2
4

€aZ
Kubernetes support by Cloud providers bytes

Kubernetes is so modular, flexible, and extensible that it can be deployed on-prem, in a third-party data center, in any
of the popular cloud providers and even across multiple cloud providers.

Creating and maintaining a K8S cluster can be very challenge in on-prem. Due that many
enterprises look for the cloud providers which will make their job easy to maintain their

microservice architecture using Kubernetes. an

Below are the different famous cloud providers and their support to Kubernetes
with the different names,

* GCP - GKE (Google Kubernetes Engine)

Ec AWS - EKS (Elastic Kubernetes Service)

Ec Azure - AKS (Azure Kubernetes Service)

Kubernetes Ingress

Me What is Ingress ?

eaz
bytes

Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on
the Ingress resource. An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and

offer name-based virtual hosting.

networking.k8s.io/vl
Ingress

example-ingress

myapp.example.com

/appl
Prefix

appl-service

/app2
Prefix

app2-service

ead What is Ingress Controller ?

On its own, the Ingress resource doesn’t do anything. You need to have an Ingress controller
installed and configured in your cluster to make Ingress resources functional. Popular Ingress
controllers include Nginx Ingress, Traefik, and HAProxy Ingress. The controller watches for
Ingress resources and configures the underlying networking components accordingly.

Full list of Ingress Controllers available in the below URL,

https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/

Kubernetes Ingress bytes

K8s Ingress Controller K8s Service

accounts
service

example.com/accounts accounts pods

Ingress-managed load
balancer

BY al oe example.com/loans

Incoming Traffic

Ingress Routing Rules

Ingress Routing Rules

example.com/cards

service

o
=
2.
“
2.
“

Traffic flow in K8s Cluster with Ingress controller

Kubernetes Ingress bytes

Me Why Use Ingress ?

Ingress provides several benefits:

Single Entry Point: It allows you to configure a single entry point for multiple services, making it easier to manage external access to your
applications.

TLS/SSL Termination: Ingress can handle TLS/SSL termination, allowing you to secure your applications with SSL/TLS certificates.

Path-Based Routing: You can route traffic to different services based on the request path (e.g., /app1 goes to one service, /app2 to another).

Host-Based Routing: You can route traffic based on the requested host or domain name (e.g., app1.example.com goes to one service,
app2.example.com to another).

Load Balancing: It provides built-in load balancing for distributing traffic among multiple pods of the same service.

Annotations:

Ingress resources can be customized using annotations. Annotations allow you to configure additional settings, such as rewrite rules, custom
headers, and authentication.

Kubernetes Ingress bytes

Me Ingress Controllers vs. Service Type LoadBalancer

Ingress controllers are often compared to using Kubernetes Service resources of type LoadBalancer. While both can expose
services externally, Ingress offers more advanced routing and traffic management capabilities.

ic Types of traffic handled by Ingress Controller

Ingress traffic: Traffic entering a Kubernetes cluster
Egress traffic: Traffic exiting a Kubernetes cluster

North-south traffic: Traffic entering and exiting a Kubernetes cluster (also called ingress-egress traffic)

- - - Caz
Who handles service-service traffic ? bytes

service-to-service traffic — Traffic moving among services within a Kubernetes cluster (also called as east-west traffic). Service mesh can
handle east-west traffic efficiently.

A service mesh is a dedicated infrastructure layer for managing communication between microservices in a containerized application.
It provides a set of networking capabilities that help facilitate secure, reliable, and observable communication between services

within a distributed system.
The service mesh can provide a variety of features, such as: CMx Z \

Service discovery: This allows services to find each other without having to hard-code the
addresses in their code.

Load balancing: This ensures that traffic is distributed evenly across all instances of a service.
Circuit breaking: This prevents a service from becoming overloaded by traffic.

Fault tolerance: This ensures that services can continue to function even if some of their
dependencies fail.

Metrics and tracing: This provides visibility into the traffic flowing through the service mesh.

Security: Service mesh can secure internal service-to-service communication in a cluster
with Mutual TLS (mTLS)


eazZz
Problem that Service Mesh trying to solve bytes

MICROSERVICES LANDSCAPE INSIDE K8s CLUSTER

re = Ll = bl —_
Business logic
Security
Metrics

I I I
I I I I
l | Tracing ] |
I I I I
I I I

v Business logic |
Vv Security

v Metrics ]
v

Tracing
Resilience etc. v Resilience etc.

loans microservice container cards microservice container

| POD

POD

Each microservice has lot of code/configurations related to Security, tracing ie Developers needs to manage these changes consistently in all the

etc. which is not related to business logic. All the extra code/configurations microservices which deviates from their main focus of building the business

that is present inside each microservice will make them complex. logic.

eazy

How Service Mesh makes our life easy while building microservices bytes

MICROSERVICES LANDSCAPE INSIDE K8s CLUSTER

J Pop J Pop

loans microservice core
logic container

Security |
Metrics

Tracing I
Resilience etc.

Security |
Metrics

Tracing |
Resilience etc.

sidecar proxy container | sidecar proxy container |

I
I
I
L

A sidecar proxy container is deployed along with each service that you start

in your cluster. This is also called as Sidecar pattern. This pattern is named

Sidecar because it resembles a sidecar attached to a motorcycle. In the
pattern, the sidecar is attached to a parent application and provides
supporting features for the application. The sidecar also shares the same
lifecycle as the parent application, being created and retired alongside the

parent.

] POD

SES 6" 41

] cards microservice core |
logic container

L._— =_ = = =
ns |

| ¥ Security
v Metrics I

] v Tracing
l
I

I Vv Resilience etc.

sidecar proxy container
= Le = = =
= Ll = bl = = = bl

A sidecar is independent from its primary application in terms of runtime
environment and programming language, so you don't need to develop one
sidecar per language. Developers will also relived from developing all these

non business logic related components.

; eazy
Service mesh components bytes

A service mesh typically consists of two components:

Data plane: This is responsible for routing traffic between services. It can be implemented using proxies. Each microservice instance is

accompanied by a lightweight proxy (e.g., Envoy, Linkerd proxy) known as a sidecar. These proxies handle traffic to and from the service,
intercepting requests and responses.

Control plane: The control plane is responsible for configuring, managing, and monitoring the proxies.
It includes components like a control plane API, service discovery, and configuration management.

Here are some of the popular service meshes:

Istio

Linkerd

Consul

Kong

AWS App Mesh
Azure Service Mesh

LN NNL LN

The best service mesh for an Organization will depend on it’s specific needs and
requirements.

—s—_

, . eazy
Istio service mesh components bytes

loans mic cards microservice

core logic container core logic container |

Istio data plane


€aZ
Introduction to mutual TLS (mTLS) bytes

Mutual TLS (mTLS) represents a variant of transport layer security (TLS). TLS, which succeeded secure sockets layer (SSL), stands as the
prevailing standard for secure communication, prominently used in HTTPS. It facilitates secure communication that guarantees both
confidentiality (protection against eavesdropping) and authenticity (protection against tampering) between a server, which needs to verify its
identity to clients.

However, in scenarios where both sides require mutual identity verification, such as interactions between microservices within a Kubernetes
application, traditional TLS falls short. mTLS comes into play when both parties must mutually authenticate themselves. It enhances the
security provided by TLS by introducing mutual authentication between the two parties.

mTLS is often used in a Zero Trust security framework* to verify users, devices, and servers within an
organization. It can also help keep APIs secure.

*Zero Trust means that no user, device, or network traffic is trusted by default, an approach that helps
eliminate many security vulnerabilities.

What is TLS?

Transport Layer Security (TLS) is an encryption protocol in wide use on the Internet. TLS, which was
formerly called SSL, authenticates the server in a client-server connection and encrypts
communications between client and server so that external parties cannot spy on the
communications.

TLS is the direct successor to SSL, and all versions of SSL are now deprecated. However, it’s
common to find the term SSL describing a TLS connection. In most cases, the terms SSL and SSL/TLS
both refer to the TLS protocol and TLS certificates.


Caz
How does TLS works ? bytes

When web browsers aim to establish a secure connection with a web server, such as https://www.amazon.com, they employ the Transport Layer Security
(TLS) protocol. This not only encrypts and safeguards private communications but also validates the server's authenticity, ensuring it truly belongs to
Amazon.

Even if we've never visited www.amazon.com before, our web browsers inherently trust the site's identity right from our initial visit. This trust is enabled

by the involvement of trusted third parties (TTPs). In the context of TLS, these TTPs are known as certificate authorities (CAs), which generate and issue
X.509 digital certificates to website owners after they provide proof of domain ownership.

Certificate Authority

same inside a Web Server where Amazon code is
deployed

When browser try to access the Amazon website, it “ issue s ee x ae after ing ine
validates the certificate of Amazon with CA. Post that omain ownership. Amazon team coniitures the

only it starts sending data to backend

Secure & encrypted communication
Web Browser between browser and server will Web Server
happen

Caz
How does TLS works ? bytes

Below are the steps happens behind the scenes just before browser start sending the data to backend server,

www
Q) TCP handshake happens and connection between client and
server established
+ seem emeewe eee ewe ewe eee ew ee ew ewe we ew ew ew ew ew ew eB ew ew ew eB ee ew ew ew ee ew ew ew ew ew eee ee ee ee

------------- ~~~ ee Private Key using which any data encrypted
with public can be decrypted

Gre (3) Server responds and send public key to client
Public Key using which any one can send

Hr rrr rrr nnn nnn nnn nnn nnn nnn nnn nnn nnn nnn nn nnn nnn mn nnn nm mn mmm encrypted data which can be understand by
server which has private key

fo aad

Session ———————» Encrypted

Key Session Key

Client browser send a encrypted session key to server which it
generated using public key

Server decrypts the session key using private key fo
and save it for later use in the same session Encrvoted
~ oe eee ryP ————> __ Session Key
+ Session Key

derived based on the session key. We called this as symmetric encryption

(6) Both client and server send data to each other in an ecrypted format
as both parties use same key for encryption and decryption

How Is mTLS Different from TLS ? bytes

mTLS enhances the security offered by TLS by introducing mutual authentication between the client and the server. Within the framework of
mTLS, both the client and the server exchange their respective certificates and mutually confirm each other's identities prior to establishing a
secure connection.

In contrast, TLS (as well as its predecessor, SSL) exclusively offers server authentication to the client. This level of authentication suffices for
numerous scenarios where the client places trust in the server and aims to validate the server's identity before transmitting sensitive data.

In a zero trust security strategy, use mTLS for secure communication between controllable application
components, such as microservices in a cluster. One-way TLS is typical for internet clients connecting to web
services, focusing on server identification. However, with supporting tech like a service mesh, mTLS operates
outside the app and simplifies implementation.

Yet, managing many certificates in mTLS can become challenging, which is where automatic mTLS
through a service mesh helps ease certificate management complexity.

The organization implementing mTLS acts as its own certificate authority. This contrasts with
standard TLS, in which the certificate authority is an external organization that checks if the
certificate owner legitimately owns the associated domain


e€aZ
How does mTLS works ? bytes

Think like, accounts microservice container want to communicate with loans microservice container and both of them are deployed in a same Kubernetes
Cluster

With out mTLS the

communication will be over
plain HTTP

accounts

Sample flow of mTLS between two microservices

1
I
I
I
I (6) Decrypted Data
I
I

I
I
]

accounts (s) Encrypted Data

Send HTTPs/TLS
handshake |

sidecar
proxy

sidecar
proxy

----2__..

Respond back
with certificate

(4) Validate the
certificate with
CA

Service Mesh Control plane which is going to act as a CA

€aZz
Why use mTLS? bytes

Mutual Transport Layer Security (mTLS) offers several advantages in securing communication between parties:

Mutual Authentication: Both the client and server authenticate each other, enhancing security by ensuring that both parties are who they claim to be. This is
especially important in scenarios where both sides need to establish trust.

Protection Against Impersonation: It guards against man-in-the-middle attacks and impersonation attempts, as both parties present valid digital certificates,
making it difficult for malicious actors to intercept or manipulate data.

Granular Access Control: mTLS can be used to enforce fine-grained access control, allowing organizations to specify which clients are permitted to access
specific services or resources, based on the certificates they possess.

Resistance to Credential Compromise: Unlike username/password authentication, mTLS relies on cryptographic keys stored securely, making it less
susceptible to credential theft or brute-force attacks.

Simplified Key Management: In many cases, mTLS leverages digital certificates issued by trusted certificate authorities (CAs), reducing the complexity of
managing encryption keys and ensuring their validity.

Scalability: While managing certificates can be challenging at scale, tools like service meshes can automate certificate distribution and rotation, simplifying
operations in large deployments.

Compliance: mTLS helps organizations meet regulatory compliance requirements for securing sensitive data and privacy, such as those outlined in GDPR,
HIPAA, or PCI DSS.

Zero Trust Security: mTLS aligns with the principles of zero trust security by requiring verification at every step of communication, fostering a more secure
and less trusting network environment.

eaz
bytes

CONGRATULATIONS

* YOU ARE NOW A MASTER OF MICROSERVICES USING SPRING, DOCKER AND KUBERNETES *

~ e e e e e —
NAA AWS |

3% ABIGTHANKYOU *

HOPING FOR OUR PATHS TO CROSS AGAIN



